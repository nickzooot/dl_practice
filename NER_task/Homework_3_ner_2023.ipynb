{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss9WR_zbQELO"
      },
      "source": [
        "# Практическое задание 3\n",
        "\n",
        "# Named Entity Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sB3vM2GfQELQ"
      },
      "source": [
        "## Введение\n",
        "\n",
        "### Постановка задачи\n",
        "\n",
        "В этом задании вы будете решать задачу извлечения именованных сущностей (Named Entity Recognition) – одну из самых распространенных в NLP, наряду с задачей текстовой классификации.\n",
        "\n",
        "Данная задача заключается в том, что нужно классифицировать каждое слово / токен на предмет того, является ли оно частью именованной сущности (сущность может состоять из нескольких слов / токенов) или нет.\n",
        "\n",
        "Например, мы хотим извлечь имена и названия организаций. Тогда для текста\n",
        "\n",
        "    Yan    Goodfellow  works  for  Google  Brain\n",
        "\n",
        "модель должна извлечь следующую последовательность:\n",
        "\n",
        "    B-PER  I-PER       O      O    B-ORG   I-ORG\n",
        "\n",
        "где префиксы *B-* и *I-* означают начало и конец именованной сущности, *O* означает слово без тега. Такая префиксная система (*BIO*-разметка) введена, чтобы различать последовательные именованные сущности одного типа.\n",
        "Существуют и другие типы разметок, например *BILUO*, но в рамках данного практического задания сфокусируемся имеено на *BIO*.\n",
        "\n",
        "Решать NER задачу мы будем на датасете CoNLL-2003 с использованием рекуррентных сетей и моделей на базе архитектуры Transformer. Датасет CoNLL-2003 представлен в виде разметки **BIO**, где лейбл:\n",
        "- *B-{label}* - начало сущности *{label}*;\n",
        "- *I-{label}* - продолжение сущности *{label}*;\n",
        "- *O* - отсутсвие сущности.\n",
        "\n",
        "Здесь в качестве сущности *{label}* может выступать имя, географическое название или какой-то другой тип собственных имён. Подробнее с разметками можно ознакомится во вспомогательном ноутбуке.\n",
        "\n",
        "### Библиотеки\n",
        "\n",
        "Основные библиотеки:\n",
        " - [PyTorch](https://pytorch.org/)\n",
        " - [Transformers](https://github.com/huggingface/transformers)\n",
        "\n",
        "### Данные\n",
        "\n",
        "Данные лежат в архиве, который состоит из:\n",
        "\n",
        "- *train.tsv* - обучающая выборка. В каждой строке записаны: <слово / токен>, <тэг слова / токена>\n",
        "\n",
        "- *valid.tsv* - валидационная выборка, которую можно использовать для подбора гиперпарамеров и замеров качества. Имеет идентичную с train.tsv структуру.\n",
        "\n",
        "- *test.tsv* - тестовая выборка, по которой оценивается итоговое качество. Имеет идентичную с train.tsv структуру.\n",
        "\n",
        "Скачать данные можно здесь: [ссылка](https://github.com/valerapon/msu_task_3_ner)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 337,
      "metadata": {
        "id": "EpTwu2i2qRrg"
      },
      "outputs": [],
      "source": [
        "#!pip install numpy==1.23.5 scikit-learn==1.2.2 tensorboard==2.14.1 torch==2.1.0 tqdm==4.66.1 transformers==4.34.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 338,
      "metadata": {
        "id": "Thidpb9qQELS"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from collections import Counter, defaultdict, namedtuple\n",
        "from typing import Tuple, List, Dict, Any\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm, trange"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiDlmbY2QELT"
      },
      "source": [
        "Зафиксируем seed (значение 42) для воспроизводимости результатов (желательно делать **всегда**!):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 339,
      "metadata": {
        "id": "yt3ISg3aQELU"
      },
      "outputs": [],
      "source": [
        "def set_global_seed(seed: int) -> None:\n",
        "    \"\"\"\n",
        "    Set global seed for reproducibility.\n",
        "    \"\"\"\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "set_global_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhIg0ZBzQELV"
      },
      "source": [
        "Проинициализируем device (CPU / GPU) на котором будем работать (желательно **GPU**):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 340,
      "metadata": {
        "id": "rboLOv95QELV"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 340,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k3Nhd3IQELY"
      },
      "source": [
        "## Часть 1. Подготовка данных (4 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qYjOMuPQELY"
      },
      "source": [
        "Первым делом нам нужно считать данные. Давайте напишем функцию, которая на вход принимает путь до одного из conll-2003 файла и возвращает два списка:\n",
        "- список списков слов / токенов;\n",
        "- список списков тегов, который соответствует собранному списку слов / токенов.\n",
        "\n",
        "Также функция на вход принимает булевую переменную *lowercase*, которая задает чувствительность к регистру. Далее будем всё приводить к нижнему регистру (`lower=True`).\n",
        "\n",
        "P.S. Стоит держать в голове, что в некоторых ситуациях верхний регистр помогает выявлять именованные сущности. Например, у вас нет мощностей, чтобы запускать сложные модели, а задачу решать нужно быстро. В этом случае эвристическое правило: \"Большая буква в слове = именнованная сущность\" может вам помочь. Или у вас есть огромные корпусы данных, которые позволяют сохранять исходное разнообразие слов.\n",
        "\n",
        "**Задание. Реализуйте функцию read_conll2003.** **<font color='red'>(1 балл)</font>**\n",
        "\n",
        "<details>\n",
        "<summary> Подсказка </summary>\n",
        "\n",
        "*Предложения разделены пустой строкой, в конце файла также пустая строка, но не забывайте про символ `\\n`.*\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 341,
      "metadata": {
        "id": "wQdCfX2OQELZ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def read_conll2003(\n",
        "    path: str,\n",
        "    lower: bool = True,\n",
        ") -> Tuple[List[List[str]], List[List[str]]]:\n",
        "    \"\"\"\n",
        "    Prepare data in CoNNL like format.\n",
        "\n",
        "    Args:\n",
        "        path: The path to the file (str).\n",
        "        lower:  Reduce text to lowercase (bool).\n",
        "\n",
        "    Returns:\n",
        "        Function returns pair (token_seq, label_seq).\n",
        "        token_seq: The list of lists. Each internal list is\n",
        "            a sentence converted into tokens.\n",
        "        label_seq: The list of lists. All internal lists\n",
        "            contain tags corresponding to tokens from token_seq.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    token_seq: List[List[str]] = []\n",
        "    label_seq: List[List[str]] = []\n",
        "\n",
        "    # ### START CODE HERE ###\n",
        "    with open(path, 'r') as data:\n",
        "        labels, tokens = [], []\n",
        "        for s in data:\n",
        "            if s != '\\n':\n",
        "                token, label = re.search(r'([^\\s]*)\\s*([BOI]-?.*)', s).group(1, 2)\n",
        "                tokens.append(token if not lower else token.lower())\n",
        "                labels.append(label)\n",
        "            else:\n",
        "               token_seq.append(tokens)\n",
        "               label_seq.append(labels)\n",
        "               tokens, labels = [], [] \n",
        "    # ### END CODE HERE ###\n",
        "\n",
        "    return token_seq, label_seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYm8xEvFQELb"
      },
      "source": [
        "Считаем все три файла:\n",
        "- *train.tsv*\n",
        "- *valid.tsv*\n",
        "- *test.tsv*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 342,
      "metadata": {
        "id": "-inr1BPgQELb"
      },
      "outputs": [],
      "source": [
        "train_token_seq, train_label_seq = read_conll2003(\"data/train.txt\")\n",
        "valid_token_seq, valid_label_seq = read_conll2003(\"data/valid.txt\")\n",
        "test_token_seq, test_label_seq = read_conll2003(\"data/test.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOoNc1VUQELc"
      },
      "source": [
        "Посмотрим на то, что мы получили:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 343,
      "metadata": {
        "id": "HK8AcwWGQELd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eu\tB-ORG\n",
            "rejects\tO\n",
            "german\tB-MISC\n",
            "call\tO\n",
            "to\tO\n",
            "boycott\tO\n",
            "british\tB-MISC\n",
            "lamb\tO\n",
            ".\tO\n"
          ]
        }
      ],
      "source": [
        "for token, label in zip(train_token_seq[0], train_label_seq[0]):\n",
        "    print(f\"{token}\\t{label}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 344,
      "metadata": {
        "id": "K8SqDeMJjF3Y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cricket\tO\n",
            "-\tO\n",
            "leicestershire\tB-ORG\n",
            "take\tO\n",
            "over\tO\n",
            "at\tO\n",
            "top\tO\n",
            "after\tO\n",
            "innings\tO\n",
            "victory\tO\n",
            ".\tO\n"
          ]
        }
      ],
      "source": [
        "for token, label in zip(valid_token_seq[0], valid_label_seq[0]):\n",
        "    print(f\"{token}\\t{label}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 345,
      "metadata": {
        "id": "ddFE7p5kjF_p"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "soccer\tO\n",
            "-\tO\n",
            "japan\tB-LOC\n",
            "get\tO\n",
            "lucky\tO\n",
            "win\tO\n",
            ",\tO\n",
            "china\tB-PER\n",
            "in\tO\n",
            "surprise\tO\n",
            "defeat\tO\n",
            ".\tO\n"
          ]
        }
      ],
      "source": [
        "for token, label in zip(test_token_seq[0], test_label_seq[0]):\n",
        "    print(f\"{token}\\t{label}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 346,
      "metadata": {
        "id": "BZ4Go3IXfDit"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Тесты пройдены!\n"
          ]
        }
      ],
      "source": [
        "assert len(train_token_seq) == len(train_label_seq), \"Длины тренировочных token_seq и label_seq не совпадают, ошибка в функции read_conll2003\"\n",
        "assert len(valid_token_seq) == len(valid_label_seq), \"Длины валидационных token_seq и label_seq не совпадают, ошибка в функции read_conll2003\"\n",
        "assert len(test_token_seq) == len(test_label_seq), \"Длины тестовых token_seq и label_seq не совпадают, ошибка в функции read_conll2003\"\n",
        "\n",
        "assert train_token_seq[0] == ['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.'], \"Ошибка в тренировочном token_seq\"\n",
        "assert train_label_seq[0] == ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O'], \"Ошибка в тренировочном label_seq\"\n",
        "\n",
        "assert valid_token_seq[0] == ['cricket', '-', 'leicestershire', 'take', 'over', 'at', 'top', 'after', 'innings', 'victory', '.'], \"Ошибка в валидационном token_seq\"\n",
        "assert valid_label_seq[0] == ['O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], \"Ошибка в валидационном label_seq\"\n",
        "\n",
        "assert test_token_seq[0] == ['soccer', '-', 'japan', 'get', 'lucky', 'win', ',', 'china', 'in', 'surprise', 'defeat', '.'], \"Ошибка в тестовом token_seq\"\n",
        "assert test_label_seq[0] == ['O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O'], \"Ошибка в тестовом label_seq\"\n",
        "\n",
        "print(\"Тесты пройдены!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SVL4USbQELe"
      },
      "source": [
        "### Подготовка словарей\n",
        "\n",
        "Чтобы обучать нейронную сеть, мы будем использовать два отображения:\n",
        "- {**token**}→{**token_idx**}: соответствие между словом / токеном и индексом строки в *embedding* матрице (начинается с 0);\n",
        "- {**label**}→{**label_idx**}: соответствие между тегом и уникальным индексом (начинается с 0).\n",
        "\n",
        "Теперь нам необходимо реализовать две функции:\n",
        "- *get_token2idx*\n",
        "- *get_label2idx*\n",
        "\n",
        "которые будут возвращать соответствующие словари (*token2idx* и *label2idx*).\n",
        "\n",
        "Словарь *token2idx* должен содержать специальные токены, которые нужно добавить самим:\n",
        "- `<PAD>` - спецтокен для паддинга (отступа), так как мы собираемся обучать модели батчами. Токен `<PAD>` нужен для выравнивания предложений по длине, когда их будем помещать в один батч. Чаще всего предложения дополняются с конца;\n",
        "- `<UNK>` - спецтокен для обработки слов / токенов, которых нет в словаре (актуально для инференса).\n",
        "\n",
        "Давайте для удобства дадим токену `<PAD>` индекс `0`, а токену `<UNK>` индекс `1`.\n",
        "\n",
        "В функцию *get_token2idx* также необходимо добавить параметр *min_count*, который будет включать только слова, превышающие определенную частоту."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSBooA__tPBP"
      },
      "source": [
        "Для начала посмотрим, а сколько вообще уникальных слов в обучающих данных и число их вхождений:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 347,
      "metadata": {
        "id": "IthnXKsoo7A3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('the', 8390)\n",
            "('.', 7374)\n",
            "(',', 7290)\n",
            "('of', 3815)\n",
            "('in', 3621)\n",
            "('to', 3424)\n",
            "('a', 3199)\n",
            "('and', 2872)\n",
            "('(', 2861)\n",
            "(')', 2861)\n",
            "Количество уникальных слов в тренировочном датасете: 21010\n",
            "Количество слов встречающихся только один раз в тренировочном датасете: 10060\n"
          ]
        }
      ],
      "source": [
        "token_counter = Counter([token for sentence in train_token_seq for token in sentence])\n",
        "print(*token_counter.most_common(10), sep='\\n')\n",
        "print(f\"Количество уникальных слов в тренировочном датасете: {len(token_counter)}\")\n",
        "print(f\"Количество слов встречающихся только один раз в тренировочном датасете: {len([token for token, cnt in token_counter.items() if cnt == 1])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRtCHt1QruSU"
      },
      "source": [
        "Как мы видим, у нас есть много слов, которые в обучении встречаются только один раз. Очевидно, что выучиться по ним у нас не получиться, мы только переобучимся, поэтому давайте выкинем такие слова при формировании нашего словаря, задав параметр функции `min_count=2`.\n",
        "\n",
        "На этом этапе можно применять различные методы сокращения размера словаря, преобразовывая разные словоформы одного слова в один токен: стемминг или лемматизация. В текущей задаче мы это опустим, но в некоторых ситуациях это бывает полезно."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOnc3UHpQELf"
      },
      "source": [
        "**Задание. Реализуйте функции get_token2idx и get_label2idx.** **<font color='red'>(1 балл)</font>**\n",
        "\n",
        "<details>\n",
        "<summary> Подсказка №1 </summary>\n",
        "\n",
        "*Не забудьте, что у \\<PAD\\> индекс 0, а у \\<UNK\\> индекс 1.*\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary> Подсказка №2 </summary>\n",
        "\n",
        "*Лучше всего словари token2idx и label2idx собирать с помощью token2cnt в get_token2idx и label_list в get_label2idx соответственно.*\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary> Подсказка №3 </summary>\n",
        "\n",
        "*В label_list в get_label2idx лежат отсортированные по возрастанию тэги, за исключением тэга 'O' – он идет первый (индекс 0). Этот порядок нужно сохранить при индексации в label2idx.*\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 348,
      "metadata": {
        "id": "aCaPftCyQELi"
      },
      "outputs": [],
      "source": [
        "def get_token2idx(\n",
        "    token_seq: List[List[str]],\n",
        "    min_count: int,\n",
        ") -> Dict[str, int]:\n",
        "    \"\"\"\n",
        "    Get mapping from tokens to indices to use with Embedding layer.\n",
        "\n",
        "    Args:\n",
        "        token_seq: The list of lists. Each internal list (sentence)\n",
        "            consists of tokens.\n",
        "        min_count:  The minimum number of repetitions of\n",
        "            a token in the corpus.\n",
        "\n",
        "    Returns:\n",
        "        Function returns mapping from token to id.\n",
        "        token2idx: The mapping from token\n",
        "            to id without \"rare\" words.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    token2idx: Dict[str, int] = {}\n",
        "    token2cnt = Counter([token for sentence in token_seq for token in sentence])\n",
        "\n",
        "    # ### START CODE HERE ###\n",
        "    token2idx = {\n",
        "        '<PAD>': 0, \n",
        "        '<UNK>': 1\n",
        "    } \n",
        "    other_token2idx = {\n",
        "        token: idx \n",
        "        for idx, token in enumerate(filter(lambda tok: token2cnt[tok] >= min_count, token2cnt), start=2) \n",
        "    }\n",
        "    token2idx.update(other_token2idx)\n",
        "    # ### END CODE HERE ###\n",
        "    return token2idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 349,
      "metadata": {
        "id": "uFK130y-sLH4"
      },
      "outputs": [],
      "source": [
        "token2idx = get_token2idx(train_token_seq, min_count=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 350,
      "metadata": {
        "id": "t6i51GPtQELj"
      },
      "outputs": [],
      "source": [
        "def get_label2idx(label_seq: List[List[str]]) -> Dict[str, int]:\n",
        "    \"\"\"\n",
        "    Get mapping from labels to indices.\n",
        "\n",
        "    Args:\n",
        "        label_seq: The list of lists. Each internal list (sentence)\n",
        "            consists of labels.\n",
        "\n",
        "    Returns:\n",
        "        Function returns mapping from label to id.\n",
        "        label2idx: The mapping from label to id.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    label2idx: Dict[str, int] = {}\n",
        "    label_list = set(label for sentence in label_seq for label in sentence)\n",
        "    label_list = sorted(label_list, key=lambda x: 'A' if x == 'O' else x)\n",
        "\n",
        "    # ### START CODE HERE ###\n",
        "    label_dict = dict(zip(label_list, range(len(label_list))))\n",
        "    label2idx = {\n",
        "        label: label_dict[label]\n",
        "        for seq in label_seq \n",
        "        for label in seq\n",
        "    }\n",
        "    # ### END CODE HERE ###\n",
        "\n",
        "    return label2idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 351,
      "metadata": {
        "id": "XW6fK0HtQELk"
      },
      "outputs": [],
      "source": [
        "label2idx = get_label2idx(train_label_seq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U13l-2IOQELk"
      },
      "source": [
        "Посмотрим на то, что мы получили:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 352,
      "metadata": {
        "id": "O7U7bMrHQELl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<PAD>\t0\n",
            "<UNK>\t1\n",
            "eu\t2\n",
            "german\t3\n",
            "call\t4\n",
            "to\t5\n",
            "boycott\t6\n",
            "british\t7\n",
            "lamb\t8\n",
            ".\t9\n"
          ]
        }
      ],
      "source": [
        "for token, idx in list(token2idx.items())[:10]:\n",
        "    print(f\"{token}\\t{idx}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 353,
      "metadata": {
        "id": "Hp75V-o2QELl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "B-ORG\t3\n",
            "O\t0\n",
            "B-MISC\t2\n",
            "B-PER\t4\n",
            "I-PER\t8\n",
            "B-LOC\t1\n",
            "I-ORG\t7\n",
            "I-MISC\t6\n",
            "I-LOC\t5\n"
          ]
        }
      ],
      "source": [
        "for label, idx in label2idx.items():\n",
        "    print(f\"{label}\\t{idx}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 354,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('<PAD>', 0),\n",
              " ('<UNK>', 1),\n",
              " ('eu', 2),\n",
              " ('german', 3),\n",
              " ('call', 4),\n",
              " ('to', 5),\n",
              " ('boycott', 6),\n",
              " ('british', 7),\n",
              " ('lamb', 8),\n",
              " ('.', 9)]"
            ]
          },
          "execution_count": 354,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(token2idx.items())[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 355,
      "metadata": {
        "id": "VYb4BdAUhNzk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Тесты пройдены!\n"
          ]
        }
      ],
      "source": [
        "assert len(get_token2idx(train_token_seq, min_count=1)) == 21012, \"Ошибка в длине словаря, скорее всего неверно реализован min_count\"\n",
        "assert len(token2idx) == 10952, \"Неправильная длина token2idx, скорее всего неверно реализован min_count\"\n",
        "assert len(label2idx) == 9, \"Неправильная длина label2idx\"\n",
        "\n",
        "assert list(token2idx.items())[:10] == [('<PAD>', 0), ('<UNK>', 1), ('eu', 2), ('german', 3), ('call', 4), ('to', 5), ('boycott', 6), ('british', 7), ('lamb', 8), ('.', 9)], \"Неправильно сформированный token2idx\"\n",
        "assert label2idx == {'O': 0, 'B-LOC': 1, 'B-MISC': 2, 'B-ORG': 3, 'B-PER': 4, 'I-LOC': 5, 'I-MISC': 6, 'I-ORG': 7, 'I-PER': 8}, \"Неправильно сформированный label2idx\"\n",
        "\n",
        "print(\"Тесты пройдены!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItPs1DmOQELm"
      },
      "source": [
        "### Подготовка датасета и загрузчика\n",
        "\n",
        "Обычно нейронные сети обучаются батчами. Это означает, что каждое обновление весов нейронной сети происходит на основе нескольких последовательностей. Технической деталью является необходимость дополнить все последовательности внутри батча до одной длины.\n",
        "\n",
        "Из предыдущего практического задания вы должны знать о `Dataset`'е (`torch.utils.data.Dataset`) – структура данных, которая хранит и может по индексу отдавать данные для обучения. Датасет должен наследоваться от стандартного PyTorch класса Dataset и переопределять методы `__len__` и `__getitem__`.\n",
        "\n",
        "Метод `__getitem__` должен возвращать индексированную последовательность и её теги.\n",
        "\n",
        "**Не забудьте** про `<UNK>` спецтокен для неизвестных слов!\n",
        "    \n",
        "Давайте напишем кастомный датасет под нашу задачу, который на вход (метод `__init__`) будет принимать:\n",
        "- *token_seq* – список списков слов / токенов;\n",
        "- *label_seq* – список списков тегов;\n",
        "- *token2idx* – отображение токена в индекс;\n",
        "- *label2idx* – отображение тега в индекс.\n",
        "\n",
        "и возвращать из метода `__getitem__` два Int64 тензора (`torch.LongTensor`) из индексов слов / токенов в сэмпле и индексов соответвующих тегов:\n",
        "\n",
        "**Задание. Реализуйте класс датасета NERDataset.** **<font color='red'>(1 балл)</font>**\n",
        "\n",
        "<details>\n",
        "<summary> Подсказка </summary>\n",
        "\n",
        "*Для понимания, зачем нужен вообще этот Dataset. У нас по факту есть \"сырые\" последовательности token_seq и label_seq, которые по индексу будут возвращать и нужный набор токенов, и соответствующий список тегов. Но для обучения это неудобно, потому что каждый раз нам необходимо конвертировать токены и теги в индексы. Инструмент Dataset нужен для того, чтобы, не задумываясь, извлекать элементы в нужном формате автоматически, как из массива.*\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 356,
      "metadata": {
        "id": "kdZvnUUpQELm"
      },
      "outputs": [],
      "source": [
        "class NERDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for NER.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        token_seq: List[List[str]],\n",
        "        label_seq: List[List[str]],\n",
        "        token2idx: Dict[str, int],\n",
        "        label2idx: Dict[str, int],\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Constructor of NERDataset class.\n",
        "\n",
        "        Args:\n",
        "            token_seq: The list of lists. Each internal list (sentence)\n",
        "                consists of tokens.\n",
        "            label_seq: The list of lists. Each internal list (sentence)\n",
        "                consists of labels.\n",
        "            token2idx: The mapping from token to id.\n",
        "            label2idx: The mapping from label to id.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "\n",
        "        \"\"\"\n",
        "        self.token2idx = token2idx\n",
        "        self.label2idx = label2idx\n",
        "\n",
        "        self.token_seq = [self.process_tokens(tokens, token2idx) for tokens in token_seq]\n",
        "        self.label_seq = [self.process_labels(labels, label2idx) for labels in label_seq]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Get dataset size.\n",
        "\n",
        "        Args:\n",
        "\n",
        "        Returns:\n",
        "            The method returns the length of the dataset.\n",
        "\n",
        "        \"\"\"\n",
        "        return len(self.token_seq)\n",
        "\n",
        "    def __getitem__(\n",
        "        self,\n",
        "        idx: int,\n",
        "    ) -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
        "        \"\"\"\n",
        "        Get an element from a dataset by index.\n",
        "        Recomendation: use LongTensor.\n",
        "\n",
        "        Args:\n",
        "            idx: Index of element (int).\n",
        "\n",
        "        Returns:\n",
        "            The method returns required element. From\n",
        "            self.token_seq and self.label_seq.\n",
        "\n",
        "        \"\"\"\n",
        "        token_ids = None\n",
        "        label_ids = None\n",
        "\n",
        "        # ### START CODE HERE ###\n",
        "        token_ids = torch.LongTensor(self.token_seq[idx])\n",
        "        label_ids = torch.LongTensor(self.label_seq[idx])\n",
        "        # ### END CODE HERE ###\n",
        "        return token_ids, label_ids\n",
        "\n",
        "    @staticmethod\n",
        "    def process_tokens(\n",
        "        tokens: List[str],\n",
        "        token2idx: Dict[str, int],\n",
        "        unk: str = \"<UNK>\",\n",
        "    ) -> List[int]:\n",
        "        \"\"\"\n",
        "        Transform list of tokens into list of tokens' indices.\n",
        "\n",
        "        Args:\n",
        "            tokens: The list (sentence) of tokens.\n",
        "            token2idx: The mapping from token to id.\n",
        "            unk: Name of <UNK> token (str).\n",
        "\n",
        "        Returns:\n",
        "            token_ids: The list of indices. Each index\n",
        "                corresponds to a token from the tokens.\n",
        "\n",
        "        \"\"\"\n",
        "        token_ids: List[int] = []\n",
        "\n",
        "        # ### START CODE HERE ###\n",
        "        token_ids = [token2idx[token if token in token2idx else '<UNK>'] for token in tokens]\n",
        "        # ### END CODE HERE ###\n",
        "        return token_ids\n",
        "\n",
        "    @staticmethod\n",
        "    def process_labels(\n",
        "        labels: List[str],\n",
        "        label2idx: Dict[str, int],\n",
        "    ) -> List[int]:\n",
        "        \"\"\"\n",
        "        Transform list of labels into list of labels' indices.\n",
        "\n",
        "        Args:\n",
        "            labels: The list of labels.\n",
        "            label2idx: The mapping from label to id.\n",
        "\n",
        "        Returns:\n",
        "            label_ids: The list of indices. Each index\n",
        "                corresponds to a label from the labels.\n",
        "\n",
        "        \"\"\"\n",
        "        label_ids = List[int]\n",
        "\n",
        "        # ### START CODE HERE ###\n",
        "        label_ids = [label2idx[label] for label in labels]\n",
        "        # ### END CODE HERE ###\n",
        "        return label_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCvaPJERQELn"
      },
      "source": [
        "Создадим три датасета:\n",
        "- *train_dataset*\n",
        "- *valid_dataset*\n",
        "- *test_dataset*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 357,
      "metadata": {
        "id": "bUMsSNkoQELn"
      },
      "outputs": [],
      "source": [
        "train_dataset = NERDataset(\n",
        "    token_seq=train_token_seq,\n",
        "    label_seq=train_label_seq,\n",
        "    token2idx=token2idx,\n",
        "    label2idx=label2idx,\n",
        ")\n",
        "valid_dataset = NERDataset(\n",
        "    token_seq=valid_token_seq,\n",
        "    label_seq=valid_label_seq,\n",
        "    token2idx=token2idx,\n",
        "    label2idx=label2idx,\n",
        ")\n",
        "test_dataset = NERDataset(\n",
        "    token_seq=test_token_seq,\n",
        "    label_seq=test_label_seq,\n",
        "    token2idx=token2idx,\n",
        "    label2idx=label2idx,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQIq1pAWQELo"
      },
      "source": [
        "Посмотрим на то, что мы получили:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 358,
      "metadata": {
        "id": "wDlhYu-YW0s3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([2, 1, 3, 4, 5, 6, 7, 8, 9]), tensor([3, 0, 2, 0, 0, 0, 2, 0, 0]))"
            ]
          },
          "execution_count": 358,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 359,
      "metadata": {
        "id": "CCURUIeKW3DG"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([1737,  571, 1777,  197,  687,  145,  349,  111, 1819, 1558,    9]),\n",
              " tensor([0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0]))"
            ]
          },
          "execution_count": 359,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "valid_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 360,
      "metadata": {
        "id": "zyAazaLzjQ-K"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([1516,  571, 1434, 1729, 4893, 2014,   67,  310,  215, 3157, 3139,    9]),\n",
              " tensor([0, 0, 1, 0, 0, 0, 0, 4, 0, 0, 0, 0]))"
            ]
          },
          "execution_count": 360,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 361,
      "metadata": {
        "id": "Gox6uyF2idwZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Тесты пройдены!\n"
          ]
        }
      ],
      "source": [
        "assert len(train_dataset) == 14986, \"Неправильная длина train_dataset\"\n",
        "assert len(valid_dataset) == 3465, \"Неправильная длина valid_dataset\"\n",
        "assert len(test_dataset) == 3683, \"Неправильная длина test_dataset\"\n",
        "\n",
        "assert torch.equal(train_dataset[0][0], torch.tensor([2,1,3,4,5,6,7,8,9])), \"Неправильно сформированный train_dataset\"\n",
        "assert torch.equal(train_dataset[0][1], torch.tensor([3,0,2,0,0,0,2,0,0])), \"Неправильно сформированный train_dataset\"\n",
        "\n",
        "assert torch.equal(valid_dataset[0][0], torch.tensor([1737,571,1777,197,687,145,349,111,1819,1558,9])), \"Неправильно сформированный valid_dataset\"\n",
        "assert torch.equal(valid_dataset[0][1], torch.tensor([0,0,3,0,0,0,0,0,0,0,0])), \"Неправильно сформированный valid_dataset\"\n",
        "\n",
        "assert torch.equal(test_dataset[0][0], torch.tensor([1516,571,1434,1729,4893,2014,67,310,215,3157,3139,9])), \"Неправильно сформированный test_dataset\"\n",
        "assert torch.equal(test_dataset[0][1], torch.tensor([0,0,1,0,0,0,0,4,0,0,0,0])), \"Неправильно сформированный test_dataset\"\n",
        "\n",
        "print(\"Тесты пройдены!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWjJuAk7QELp"
      },
      "source": [
        "Для того, чтобы дополнять последовательности паддингом, будем использовать параметр `collate_fn` класса `DataLoader`.\n",
        "\n",
        "Принимая последовательность пар тензоров для предложений и тегов, необходимо дополнить все последовательности до последовательности максимальной длины в батче.\n",
        "\n",
        "Используйте для дополнения спецтокен `<PAD>` для последовательностей слов / токенов и -1 для последовательностей тегов.\n",
        "\n",
        "**hint**: удобно использовать метод **torch.nn.utils.rnn**. Обратите особое внимание на параметр *batch_first*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZiJVM5qQELp"
      },
      "source": [
        "`Collator` можно реализовать двумя способами:\n",
        "- класс с методом `__call__`\n",
        "- функцию\n",
        "\n",
        "Мы пойдем первым путем.\n",
        "\n",
        "Инициализировать экземпляр класса `Collator` (метод `__init__`) с помощью двух параметров:\n",
        "- id `<PAD>` спецтокена для последовательностей слов / токенов\n",
        "- id `<PAD>` спецтокена для последовательностей тегов (значение -1)\n",
        "\n",
        "Метод `__call__` на вход принимает батч, а именно список кортежей того, что нам возвращается из датасета. В нашем случае это список кортежей двух int64 тензоров - `List[Tuple[torch.LongTensor, torch.LongTensor]]`.\n",
        "\n",
        "На выходе мы хотим получить два тензора:\n",
        "- западденные индексы слов / токенов\n",
        "- западденные индексы тегов\n",
        "    \n",
        "P.S. `<PAD>` значение нужно для того, чтобы при подсчете лосса легко отличать западдированные токены от других. Можно использовать параметр *ignore_index* при инициализации лосса.\n",
        "\n",
        "**Задание. Реализуйте класс коллатора NERCollator.** **<font color='red'>(1 балл)</font>**\n",
        "\n",
        "<details>\n",
        "<summary> Пояснение </summary>\n",
        "\n",
        "*Чтобы ускорить взаимодействие с нейронными сетями, им на вход подаются не одна пара (объект, ответ), а несколько. Они объединяются в одну связку, которую называют батч. В нашем случае объекты -- это последовательности слов, которые имеют разную длину. Мы не можем в одном pytorch.tensor (или numpy.array) хранить последовательности разных длин, поэтому требуется их привести к одной (например, к длине максимальной последовательности), чем коллатор и занимается.*\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 362,
      "metadata": {
        "id": "LNHNwoLnQELp"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "class NERCollator:\n",
        "    \"\"\"\n",
        "    Collator that handles variable-size sentences.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        token_padding_value: int,\n",
        "        label_padding_value: int,\n",
        "    ):\n",
        "        self.token_padding_value = token_padding_value\n",
        "        self.label_padding_value = label_padding_value\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        batch: List[Tuple[torch.LongTensor, torch.LongTensor]],\n",
        "    ) -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
        "        \"\"\"\n",
        "        The method appends <PAD> token id to tensor with\n",
        "        token ids and -1 to tensor with labels.\n",
        "        The function torch.nn.utils.rnn.pad_sequence is useful for padding. Link:\n",
        "        https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html\n",
        "\n",
        "        Args:\n",
        "            batch: The list of tuples. Each tuple conists of the tensor with token ids and\n",
        "                the tensor with label ids.\n",
        "\n",
        "        Returns:\n",
        "            (tokens, labels): a pair of tensors with sizes: (batch_size, sentence_len).\n",
        "\n",
        "        \"\"\"\n",
        "        tokens, labels = zip(*batch)\n",
        "\n",
        "        # ### START CODE HERE ###\n",
        "        tokens = pad_sequence(tokens, padding_value=self.token_padding_value, batch_first=True)\n",
        "        labels = pad_sequence(labels, padding_value = self.label_padding_value, batch_first=True)\n",
        "        # ### END CODE HERE ###\n",
        "\n",
        "        return tokens, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 363,
      "metadata": {
        "id": "nZUMwVQTQELq"
      },
      "outputs": [],
      "source": [
        "collator = NERCollator(\n",
        "    token_padding_value=token2idx[\"<PAD>\"],\n",
        "    label_padding_value=-1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jsgfij8WQELq"
      },
      "source": [
        "Теперь всё готово, чтобы задать `DataLoader`'ы:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 364,
      "metadata": {
        "id": "gFljkiBOQELr"
      },
      "outputs": [],
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        "    collate_fn=collator,\n",
        ")\n",
        "valid_dataloader = torch.utils.data.DataLoader(\n",
        "    valid_dataset,\n",
        "    batch_size=1,  # для корректных замеров метрик оставить batch_size=1\n",
        "    shuffle=False, # для корректных замеров метрик оставить shuffle=False\n",
        "    collate_fn=collator,\n",
        ")\n",
        "test_dataloader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=1,  # для корректных замеров метрик оставить batch_size=1\n",
        "    shuffle=False, # для корректных замеров метрик оставить shuffle=False\n",
        "    collate_fn=collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i34wGJ4uQELr"
      },
      "source": [
        "Посмотрим на то, что мы получили:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 365,
      "metadata": {
        "id": "QLlr_DztQELr"
      },
      "outputs": [],
      "source": [
        "tokens, labels = next(iter(train_dataloader))\n",
        "\n",
        "tokens = tokens.to(device)\n",
        "labels = labels.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 366,
      "metadata": {
        "id": "zQvYRqEFtmpz"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[7796, 1162, 2553, 7237, 1342,    0,    0,    0,    0,    0],\n",
              "        [ 125, 1167,    1,   67, 1349,  489, 1215, 1364, 1365, 1366]],\n",
              "       device='cuda:0')"
            ]
          },
          "execution_count": 366,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 367,
      "metadata": {
        "id": "w--fhADKQELs"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 3,  0,  3,  7,  0, -1, -1, -1, -1, -1],\n",
              "        [ 0,  4,  8,  0,  1,  0,  0,  0,  0,  0]], device='cuda:0')"
            ]
          },
          "execution_count": 367,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 368,
      "metadata": {
        "id": "yFeX0AYKlhGk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Тесты пройдены!\n"
          ]
        }
      ],
      "source": [
        "train_tokens, train_labels = next(iter(\n",
        "    torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=2,\n",
        "        shuffle=False,\n",
        "        collate_fn=collator,\n",
        "    )\n",
        "))\n",
        "assert torch.equal(train_tokens, torch.tensor([[ 2,  1,  3,  4,  5,  6,  7,  8,  9], [10, 11,  0,  0,  0,  0,  0,  0,  0]])), \"Похоже на ошибку в коллаторе\"\n",
        "assert torch.equal(train_labels, torch.tensor([[ 3,  0,  2,  0,  0,  0,  2,  0,  0], [ 4,  8, -1, -1, -1, -1, -1, -1, -1]])), \"Похоже на ошибку в коллаторе\"\n",
        "\n",
        "valid_tokens, valid_labels = next(iter(\n",
        "    torch.utils.data.DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=2,\n",
        "        shuffle=False,\n",
        "        collate_fn=collator,\n",
        "    )\n",
        "))\n",
        "assert torch.equal(valid_tokens, torch.tensor([[ 1737,   571,  1777,   197,   687,   145,   349,   111,  1819,  1558, 9], [  248, 10679,     0,     0,     0,     0,     0,     0,     0,     0,    0]])), \"Похоже на ошибку в коллаторе\"\n",
        "assert torch.equal(valid_labels, torch.tensor([[ 0,  0,  3,  0,  0,  0,  0,  0,  0,  0,  0], [ 1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1]])), \"Похоже на ошибку в коллаторе\"\n",
        "\n",
        "test_tokens, test_labels = next(iter(\n",
        "    torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=2,\n",
        "        shuffle=False,\n",
        "        collate_fn=collator,\n",
        "    )\n",
        "))\n",
        "assert torch.equal(test_tokens, torch.tensor([[1516,  571, 1434, 1729, 4893, 2014,   67,  310,  215, 3157, 3139,    9], [   1,    1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]])), \"Похоже на ошибку в коллаторе\"\n",
        "assert torch.equal(test_labels, torch.tensor([[ 0,  0,  1,  0,  0,  0,  0,  4,  0,  0,  0,  0], [ 4,  8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]])), \"Похоже на ошибку в коллаторе\"\n",
        "\n",
        "print(\"Тесты пройдены!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ul5gLriQELs"
      },
      "source": [
        "## Часть 2. BiLSTM-теггер (6 баллов)\n",
        "\n",
        "Определите архитектуру сети, используя библиотеку PyTorch.\n",
        "\n",
        "Ваша архитектура в этом пункте должна соответствовать стандартному теггеру:\n",
        "* Embedding слой на входе\n",
        "* LSTM (однонаправленный или двунаправленный) слой для обработки последовательности\n",
        "* Dropout (заданный отдельно или встроенный в LSTM) для уменьшения переобучения\n",
        "* Linear слой на выходе\n",
        "\n",
        "Для обучения сети используйте поэлементную кросс-энтропийную функцию потерь.\n",
        "\n",
        "**Обратите внимание**, что `<PAD>` токены не должны учавствовать в подсчёте функции потерь. В качестве оптимизатора рекомендуется использовать Adam. Для получения значений предсказаний по выходам модели используйте функцию `argmax`.\n",
        "\n",
        "**Задание. Реализуйте класс модели BiLSTM.** **<font color='red'>(2 балл)</font>**\n",
        "\n",
        "<details>\n",
        "<summary> Подсказка №1 </summary>\n",
        "\n",
        "*Помните, что следуется явно указывать параметр `batch_first=True`.*\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary> Подсказка №2 </summary>\n",
        "\n",
        "*Число входных признаков `in_features` в линейном слое зависим от типа LSTM: однонаправленная или двунаправленая. В первом случае выходом i-го блока является скрытое состояние $h_i$, которое имеет размер `hidden_size`, и соответственно, `in_features=hidden_size`. А в случае двунаправленной сети вход представим в виде конкатенации скрытых состояний из разных уровней сети: $[h_i^1, h_i^2]$, здесь `in_features=2 * hidden_size`.*\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary> Рекомендация </summary>\n",
        "\n",
        "*Обратите внимание на метод `BiLSTM.forward`, реализовывать самостоятельно его не нужно. В нем используется функция `pack_padded_sequence` и обратная ей `pad_packed_sequence`. Это удобный инструмент для фильтрации входных последовательностей от токенов отступа.*\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 369,
      "metadata": {
        "id": "uMiLQljZQELt"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Embedding, LSTM, Linear\n",
        "\n",
        "\n",
        "class BiLSTM(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Bidirectional LSTM architecture.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_embeddings: int,\n",
        "        embedding_dim: int,\n",
        "        hidden_size: int,\n",
        "        num_layers: int,\n",
        "        dropout: float,\n",
        "        bidirectional: bool,\n",
        "        n_classes: int,\n",
        "        token_padding_value: int,\n",
        "        max_norm: float,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        BiLSTM class constructor. The method contains a description\n",
        "        of the network layers and their parameters.\n",
        "\n",
        "        Args:\n",
        "            num_embeddings: The number of unique words (tokens) in the\n",
        "                dictionary, or the size of the dictionary (int)\n",
        "            embedding_dim:  Embedding word size (size)\n",
        "            hidden_size: The size of the hidden state (h_n) in the LSTM network (int)\n",
        "            num_layers: The total number of blocks in an LSTM network (int)\n",
        "            dropout: dropout parameter in LSTM layers (float)\n",
        "            bidirectional: a parameter that controls whether the LSTM is\n",
        "                unidirectional (one direction) or bidirectional (bool)\n",
        "            n_classes: the number of classes in a problem being solved (int)\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "\n",
        "        Tips:\n",
        "            - do not forget to specify batch_first=True\n",
        "            - control the size of the in_features in linear\n",
        "                layer depending on the value of 'bidirectional'\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_padding_value = token_padding_value\n",
        "        self.embedding = None # Embedding layer\n",
        "        self.rnn = None # LSTM layer\n",
        "        self.head = None # Linear layer\n",
        "\n",
        "        # ### START CODE HERE ###\n",
        "        self.embedding = torch.nn.Embedding(num_embeddings, embedding_dim)\n",
        "        self.rnn = torch.nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "            bidirectional=bidirectional)\n",
        "        self.head = torch.nn.Linear(hidden_size * (bidirectional + 1), n_classes)\n",
        "        # ### END CODE HERE ###\n",
        "\n",
        "    def forward(self, tokens: torch.LongTensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Applying neural network layers to input 'tokens'.\n",
        "\n",
        "        Args:\n",
        "            tokens: the input tensor with tokens ids (batch_size, sequence_len)\n",
        "\n",
        "        Returns:\n",
        "            logits: the scores issued by the model (batch_size, num_classes, sequence_len)\n",
        "        \"\"\"\n",
        "        embed = self.embedding(tokens)\n",
        "\n",
        "        # Используем специальную функцию pack_padded_sequence для того, чтобы получить\n",
        "        # структуру PackedSequence, которая не учитывать паддинг при проходе rnn.\n",
        "        # lengths -- длины исходных исходных последовательностей в батче,\n",
        "        # без учёта сдвига\n",
        "        lengths = (tokens != self.token_padding_value).sum(dim=1).detach().cpu()\n",
        "        packed_embed = torch.nn.utils.rnn.pack_padded_sequence(\n",
        "            input=embed,\n",
        "            lengths=lengths,\n",
        "            batch_first=True,\n",
        "            enforce_sorted=False,\n",
        "        )\n",
        "\n",
        "        # Используем специальную функцию pad_packed_sequence для того, чтобы получить\n",
        "        # тензор из PackedSequence. Операция является обратной к pack_padded_sequence\n",
        "        packed_rnn_output, _ = self.rnn(packed_embed)\n",
        "        rnn_output, _ = torch.nn.utils.rnn.pad_packed_sequence(\n",
        "            sequence=packed_rnn_output,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        logits = self.head(rnn_output)\n",
        "        logits = logits.transpose(1, 2)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 370,
      "metadata": {
        "id": "zps8HL2VQELu"
      },
      "outputs": [],
      "source": [
        "model = BiLSTM(\n",
        "    num_embeddings=len(token2idx),\n",
        "    embedding_dim=300,\n",
        "    hidden_size=256,\n",
        "    num_layers=1,\n",
        "    dropout=0.0,\n",
        "    bidirectional=True,\n",
        "    n_classes=len(label2idx),\n",
        "    token_padding_value=token2idx[\"<PAD>\"],\n",
        "    max_norm=None,\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 371,
      "metadata": {
        "id": "nmg2_C_oQELu"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BiLSTM(\n",
              "  (embedding): Embedding(10952, 300)\n",
              "  (rnn): LSTM(300, 256, batch_first=True, bidirectional=True)\n",
              "  (head): Linear(in_features=512, out_features=9, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 371,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 372,
      "metadata": {
        "id": "sDvWB5J2QELv"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 373,
      "metadata": {
        "id": "Jn5Pu1UKQELv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 9, 10])\n"
          ]
        }
      ],
      "source": [
        "outputs = model(tokens)\n",
        "print(outputs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 374,
      "metadata": {
        "id": "n02Bsh8eQELw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Тесты пройдены!\n"
          ]
        }
      ],
      "source": [
        "assert outputs.shape == torch.Size([2, 9, 10])\n",
        "assert 2 < criterion(outputs, labels) < 3\n",
        "\n",
        "print(\"Тесты пройдены!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjhkK9QFQELu"
      },
      "source": [
        "### Эксперименты\n",
        "\n",
        "Проведите эксперименты на данных. Настраивайте параметры по валидационной выборке, не используя тестовую. Ваше цель — настроить сеть так, чтобы качество модели по F1-macro мере на валидационной и тестовой выборках было не меньше **0.76**.\n",
        "\n",
        "Сделайте выводы о качестве модели, переобучении, чувствительности архитектуры к выбору гиперпараметров. Оформите результаты экспериментов в виде мини-отчета (в этом же ipython notebook)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 375,
      "metadata": {
        "id": "f4hdrFZ9iRPi"
      },
      "outputs": [],
      "source": [
        "# Создадим SummaryWriter для эксперимента с BiLSTMModel\n",
        "# для отслеживания процесса обучения нейронной сети\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "writer = SummaryWriter(log_dir=f\"logs/BiLSTMModel/dropout0.3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruMTBSkQQELx"
      },
      "source": [
        "**Задание. Реализуйте функцию подсчета метрик compute_metrics.** **<font color='red'>(1 балл)</font>**\n",
        "\n",
        "<details>\n",
        "<summary> Подсказка №1 </summary>\n",
        "\n",
        "*Модель выдает логиты, или скоры, для каждого токена по каждому классу. Для подсчета метрик нужно, подобно максимизации вероятности, каждому токену входной последовательности определять класс с максимальный скором. Пример: токен `X` получил скоры (выход модели) для четырех классов `[0.5, 10.2, -13,9, 5,5]` соответственно, следовательно, нам необходимо определять для токена `X` класс № 1 (нумерация с нуля, `score=10.2`) как наиболее \"вероятный\".*\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary> Подсказка №2 </summary>\n",
        "\n",
        "*Входные тензоры необходимо перевести на CPU (если они на GPU), конвертировать в numpy-массив и для простоты вытянуть в вектор с помощью функции `flatten`.*\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary> Подсказка №3 </summary>\n",
        "\n",
        "*Не забудьте отфильтровать `<PAD>` токен.*\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 376,
      "metadata": {
        "id": "xkpo3JgWQELx"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "def compute_metrics(\n",
        "    outputs: torch.Tensor,\n",
        "    labels: torch.LongTensor,\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Compute NER metrics.\n",
        "\n",
        "    Args:\n",
        "        outputs: the model outputs (batch_size, num_classes, sequence_len)\n",
        "        labels: the correct classes (batch_size, sequence_len)\n",
        "\n",
        "    Returns:\n",
        "        metrics: mapping metric names to their corresponding values\n",
        "    \"\"\"\n",
        "\n",
        "    metrics = {}\n",
        "    y_true = None\n",
        "    y_pred = None\n",
        "\n",
        "    # ### START CODE HERE ###\n",
        "    \n",
        "    mask = (labels != collator.label_padding_value)\n",
        "    y_pred = outputs.argmax(dim=1)[mask].cpu()\n",
        "    y_true = labels[mask].cpu()\n",
        "\n",
        "    # ### END CODE HERE ###\n",
        "\n",
        "    metrics['accuracy'] = accuracy_score(\n",
        "        y_true=y_true,\n",
        "        y_pred=y_pred,\n",
        "    )\n",
        "\n",
        "    for metric_func in [precision_score, recall_score, f1_score]:\n",
        "        metric_name = metric_func.__name__.split('_')[0]\n",
        "        for average_type in [\"micro\", \"macro\", \"weighted\"]:\n",
        "            metrics[metric_name + '_' + average_type] = metric_func(\n",
        "                y_true=y_true,\n",
        "                y_pred=y_pred,\n",
        "                average=average_type,\n",
        "                zero_division=0,\n",
        "            )\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 377,
      "metadata": {
        "id": "1mYqYD-LNiNh"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 0.06666666666666667,\n",
              " 'precision_micro': 0.06666666666666667,\n",
              " 'precision_macro': 0.0625,\n",
              " 'precision_weighted': 0.3,\n",
              " 'recall_micro': 0.06666666666666667,\n",
              " 'recall_macro': 0.013888888888888888,\n",
              " 'recall_weighted': 0.06666666666666667,\n",
              " 'f1_micro': 0.06666666666666667,\n",
              " 'f1_macro': 0.022727272727272724,\n",
              " 'f1_weighted': 0.10909090909090909}"
            ]
          },
          "execution_count": 377,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "compute_metrics(\n",
        "    outputs=outputs,\n",
        "    labels=labels,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dzj89UygQEL0"
      },
      "source": [
        "**Задание. Реализуйте функции обучения и тестирования train_epoch и evaluate_epoch.** **<font color='red'>(2 балла)</font>**\n",
        "\n",
        "<details>\n",
        "<summary> Подсказка №1 </summary>\n",
        "\n",
        "*Почти всегда шаг обучения модели остается неизменным. Нужно выполнить последовательность действий: обнулить градиент, получить выходы модели, посчитать лосс, посчитать новые градиенты через `.backward()`, сделать шаг оптимизатора.*\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary> Подсказка №2 </summary>\n",
        "\n",
        "*При evaluate-этапе никакие градиенты не вычисляются, потому нет необходимости ни их в обнулении, ни в оптимизации по ним.*\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary> Пояснение </summary>\n",
        "\n",
        "*В PyTorch градиенты аккумулируются, их нужно сбрасывать через `optimizer.zero_grad()` перед каждой новой итерацией, чтобы исключить влияние градиента с предыдущих итераций на шаг текущего.*\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 378,
      "metadata": {
        "id": "sG3vQbc_QEL0"
      },
      "outputs": [],
      "source": [
        "def train_epoch(\n",
        "    model: torch.nn.Module,\n",
        "    dataloader: torch.utils.data.DataLoader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    criterion: torch.nn.Module,\n",
        "    writer: SummaryWriter,\n",
        "    device: torch.device,\n",
        "    epoch: int,\n",
        "    model_type: str,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    One training cycle (loop).\n",
        "\n",
        "    Args:\n",
        "        model: BiLSTM model\n",
        "        dataloader: Dataloader with train data\n",
        "        optimizer: an algorithm for model optimization\n",
        "        criterion: the loss function\n",
        "        writer: a tool for logging the learning process\n",
        "        device: the device on which the model will work\n",
        "        epoch: the total number of epochs\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = []\n",
        "    batch_metrics_list = defaultdict(list)\n",
        "\n",
        "    for i, (tokens, labels) in tqdm(\n",
        "        enumerate(dataloader),\n",
        "        total=len(dataloader),\n",
        "        desc=\"loop over train batches\",\n",
        "    ):\n",
        "\n",
        "        tokens, labels = tokens.to(device), labels.to(device)\n",
        "\n",
        "        outputs = None\n",
        "        loss = None\n",
        "\n",
        "        if model_type == 'BiLSTM':\n",
        "            # ### START CODE HERE ###\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(tokens)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # ### END CODE HERE ###\n",
        "        elif model_type == 'Transformer':\n",
        "            # ### START CODE HERE ###\n",
        "            # Реализуйте этот фрагмент, когда будете проводить эксперименты\n",
        "            # с трансформером в третьей части задания, а пока можете пропусить.\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(**tokens)['logits'].transpose(1, 2)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # ### END CODE HERE ###\n",
        "        else:\n",
        "            raise ValueError('Use \\'BiLSTM\\' or \\'Transformer\\' model_type.')\n",
        "\n",
        "        epoch_loss.append(loss.item())\n",
        "        writer.add_scalar(\n",
        "            tag=\"batch loss / train\",\n",
        "            scalar_value=loss.item(),\n",
        "            global_step=epoch * len(dataloader) + i,\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            if model_type == 'BiLSTM':\n",
        "                outputs_inference = model(tokens)\n",
        "            elif model_type == 'Transformer':\n",
        "                outputs_inference = model(**tokens)[\"logits\"].transpose(1, 2)\n",
        "            else:\n",
        "                raise ValueError('Use \\'BiLSTM\\' or \\'Transformer\\' model_type.')\n",
        "            model.train()\n",
        "\n",
        "        batch_metrics = compute_metrics(\n",
        "            outputs=outputs_inference,\n",
        "            labels=labels,\n",
        "        )\n",
        "\n",
        "        for metric_name, metric_value in batch_metrics.items():\n",
        "            batch_metrics_list[metric_name].append(metric_value)\n",
        "            writer.add_scalar(\n",
        "                tag=f\"batch {metric_name} / train\",\n",
        "                scalar_value=metric_value,\n",
        "                global_step=epoch * len(dataloader) + i,\n",
        "            )\n",
        "\n",
        "    avg_loss = np.mean(epoch_loss)\n",
        "    print(f\"Train loss: {avg_loss}\\n\")\n",
        "    writer.add_scalar(\n",
        "        tag=\"loss / train\",\n",
        "        scalar_value=avg_loss,\n",
        "        global_step=epoch,\n",
        "    )\n",
        "\n",
        "    for metric_name, metric_value_list in batch_metrics_list.items():\n",
        "        metric_value = np.mean(metric_value_list)\n",
        "        print(f\"Train {metric_name}: {metric_value}\\n\")\n",
        "        writer.add_scalar(\n",
        "            tag=f\"{metric_name} / train\",\n",
        "            scalar_value=metric_value,\n",
        "            global_step=epoch,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 379,
      "metadata": {
        "id": "p4ztFogtQEL0"
      },
      "outputs": [],
      "source": [
        "def evaluate_epoch(\n",
        "    model: torch.nn.Module,\n",
        "    dataloader: torch.utils.data.DataLoader,\n",
        "    criterion: torch.nn.Module,\n",
        "    writer: SummaryWriter,\n",
        "    device: torch.device,\n",
        "    epoch: int,\n",
        "    model_type: str,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    One evaluation cycle (loop).\n",
        "\n",
        "    Args:\n",
        "        model: BiLSTM model\n",
        "        dataloader: Dataloader with data for evaluation\n",
        "        criterion: a loss function\n",
        "        writer: a tool for logging the learning process\n",
        "        device: the device on which the model will work\n",
        "        epoch: the total number of epochs\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = []\n",
        "    batch_metrics_list = defaultdict(list)\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for i, (tokens, labels) in tqdm(\n",
        "            enumerate(dataloader),\n",
        "            total=len(dataloader),\n",
        "            desc=\"loop over test batches\",\n",
        "        ):\n",
        "\n",
        "            tokens, labels = tokens.to(device), labels.to(device)\n",
        "\n",
        "            if model_type == 'BiLSTM':\n",
        "                # ### START CODE HERE ###\n",
        "                outputs = model(tokens)\n",
        "                loss = criterion(outputs, labels)\n",
        "                # ### END CODE HERE ###\n",
        "            elif model_type == 'Transformer':\n",
        "                # ### START CODE HERE ###\n",
        "                # Реализуйте этот фрагмент, когда будете проводить эксперименты\n",
        "                # с трансформером в третьей части задания, а пока можете пропусить.\n",
        "                outputs = model(**tokens)['logits'].transpose(1, 2)\n",
        "                loss = criterion(outputs, labels)\n",
        "                # ### END CODE HERE ###\n",
        "            else:\n",
        "                raise ValueError('Use \\'BiLSTM\\' or \\'Transformer\\' model_type.')\n",
        "\n",
        "            epoch_loss.append(loss.item())\n",
        "            writer.add_scalar(\n",
        "                tag=\"batch loss / test\",\n",
        "                scalar_value=loss.item(),\n",
        "                global_step=epoch * len(dataloader) + i,\n",
        "            )\n",
        "\n",
        "            batch_metrics = compute_metrics(\n",
        "                outputs=outputs,\n",
        "                labels=labels,\n",
        "            )\n",
        "\n",
        "            for metric_name, metric_value in batch_metrics.items():\n",
        "                batch_metrics_list[metric_name].append(metric_value)\n",
        "                writer.add_scalar(\n",
        "                    tag=f\"batch {metric_name} / test\",\n",
        "                    scalar_value=metric_value,\n",
        "                    global_step=epoch * len(dataloader) + i,\n",
        "                )\n",
        "\n",
        "        avg_loss = np.mean(epoch_loss)\n",
        "        print(f\"Test loss:  {avg_loss}\\n\")\n",
        "        writer.add_scalar(\n",
        "            tag=\"loss / test\",\n",
        "            scalar_value=avg_loss,\n",
        "            global_step=epoch,\n",
        "        )\n",
        "\n",
        "        for metric_name, metric_value_list in batch_metrics_list.items():\n",
        "            metric_value = np.mean(metric_value_list)\n",
        "            print(f\"Test {metric_name}: {metric_value}\\n\")\n",
        "            writer.add_scalar(\n",
        "                tag=f\"{metric_name} / test\",\n",
        "                scalar_value=np.mean(metric_value),\n",
        "                global_step=epoch,\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 380,
      "metadata": {
        "id": "z7Z5MTNzQEL1"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    n_epochs: int,\n",
        "    model: torch.nn.Module,\n",
        "    train_dataloader: torch.utils.data.DataLoader,\n",
        "    valid_dataloader: torch.utils.data.DataLoader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    criterion: torch.nn.Module,\n",
        "    writer: SummaryWriter,\n",
        "    device: torch.device,\n",
        "    model_type: str,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Training loop.\n",
        "\n",
        "    Args:\n",
        "        n_epochs: the total number of epochs in training\n",
        "        model: BiLSTM model\n",
        "        train_dataloader:  Dataloader with train data\n",
        "        valid_dataloader: Dataloader with data for evaluation\n",
        "        optimizer: an algorithm for model optimization\n",
        "        criterion: a loss function\n",
        "        writer: a tool for logging the learning process\n",
        "        device: the device on which the model will work\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "        print(f\"Epoch [{epoch+1} / {n_epochs}]\\n\")\n",
        "\n",
        "        train_epoch(\n",
        "            model=model,\n",
        "            dataloader=train_dataloader,\n",
        "            optimizer=optimizer,\n",
        "            criterion=criterion,\n",
        "            writer=writer,\n",
        "            device=device,\n",
        "            epoch=epoch,\n",
        "            model_type=model_type,\n",
        "        )\n",
        "        evaluate_epoch(\n",
        "            model=model,\n",
        "            dataloader=valid_dataloader,\n",
        "            criterion=criterion,\n",
        "            writer=writer,\n",
        "            device=device,\n",
        "            epoch=epoch,\n",
        "            model_type=model_type,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTxfU0BfQEL1"
      },
      "source": [
        "**Задание. Проведите эксперименты.** **<font color='red'>(2 балла)</font>**\n",
        "\n",
        "Настало время собрать все воедино. В этом блоке предлагается подбирать разные параметры, чтобы достичь качества F1-macro на тестовой и валидационной выборках не менее **0.76**.\n",
        "\n",
        "Будем задействовать ранее определенные `test_dataloader`, `valid_dataloader`, `criretion`. Настраивайте параметры на валидационной выборке так, чтобы получить требуемое качество. Основные из них:\n",
        "- `n_epoch` -- число эпох обучения, рекомендуется выбирать от 5 до 20;\n",
        "- `embedding_dim` -- размерность эмбеддингов, рекомендуем выбирать от 8 до 526;\n",
        "- `hidden_size` -- размерность скрытого состояния, от 8 до 1024;\n",
        "- `batch_size` -- размер обучающий батчей, от 8 до 128;\n",
        "- `dropout` -- параметр регуляризатора dropout, от 0 до 0.7;\n",
        "- `max_norm` -- максимальное органичение на норму эмбеддингов, 1.0 или `None`;\n",
        "- `lr` -- шаг оптимизатора, от 1e-3 до 1e-7.\n",
        "\n",
        "На практике используют еще более широкий набор регулировок, которые в положительную сторону влияют на качество. В текущем задании они не используются, но стоит знать, что:\n",
        "\n",
        "- в процессе обучения используют `gradient clipping`, чтобы контролировать норму градиентов. Величина должна быть согласована с `max_norm`, если такая используется;\n",
        "- изменение `lr` в процессе обучения, например, уменьшение с каждой эпохой. В трансформерах это отдельная проблема, которая при неправильном выборе `lr` приводит к серьезному переобучению. Типичной практикой является использование планировщиков `lr`: https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate;\n",
        "- иногда выбор другого оптимизатора позволяет поднять качество: https://pytorch.org/docs/stable/optim.html#algorithms;\n",
        "- встаивают дополнительные регуляризационные блоки и регуляризационные механизмы, например, L2-норму.\n",
        "\n",
        "<details>\n",
        "<summary> Подсказка №1 </summary>\n",
        "\n",
        "*Следите за лоссом и метриками. Если в течение первых пяти эпохах нет роста качества, то скорее всего что-то не так.*\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary> Подсказка №2 </summary>\n",
        "\n",
        "*Подсказка 2: попробуйте начать с параметров `embedding_dim=300` и `hidden_size=256`, `dropout=0.0`*\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 381,
      "metadata": {
        "id": "3gfxTDI3M3jQ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/nickzooot/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        }
      ],
      "source": [
        "# ### START CODE HERE ###\n",
        "train_dataloader = train_dataloader\n",
        "\n",
        "model = BiLSTM(\n",
        "    num_embeddings=len(token2idx),\n",
        "    embedding_dim=300,\n",
        "    hidden_size=256,\n",
        "    num_layers=1,\n",
        "    dropout=0.3,\n",
        "    bidirectional=True,\n",
        "    n_classes=len(label2idx),\n",
        "    token_padding_value=token2idx[\"<PAD>\"],\n",
        "    max_norm=None,\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "# ### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 382,
      "metadata": {
        "id": "yz6mjGZUQEL2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1 / 10]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over train batches: 100%|██████████| 7493/7493 [00:46<00:00, 160.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.5023035939047233\n",
            "\n",
            "Train accuracy: 0.8684820213542512\n",
            "\n",
            "Train precision_micro: 0.8684820213542512\n",
            "\n",
            "Train precision_macro: 0.5220018828388604\n",
            "\n",
            "Train precision_weighted: 0.8025780554669174\n",
            "\n",
            "Train recall_micro: 0.8684820213542512\n",
            "\n",
            "Train recall_macro: 0.5303921913482064\n",
            "\n",
            "Train recall_weighted: 0.8684820213542512\n",
            "\n",
            "Train f1_micro: 0.8684820213542512\n",
            "\n",
            "Train f1_macro: 0.5162763561082876\n",
            "\n",
            "Train f1_weighted: 0.8268588145520623\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over test batches: 100%|██████████| 3465/3465 [00:13<00:00, 266.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss:  0.32845711628837554\n",
            "\n",
            "Test accuracy: 0.9082153526769019\n",
            "\n",
            "Test precision_micro: 0.9082153526769019\n",
            "\n",
            "Test precision_macro: 0.7359524064554974\n",
            "\n",
            "Test precision_weighted: 0.8735546886254475\n",
            "\n",
            "Test recall_micro: 0.9082153526769019\n",
            "\n",
            "Test recall_macro: 0.7407682843543566\n",
            "\n",
            "Test recall_weighted: 0.9082153526769019\n",
            "\n",
            "Test f1_micro: 0.9082153526769019\n",
            "\n",
            "Test f1_macro: 0.7310352822988874\n",
            "\n",
            "Test f1_weighted: 0.8845748051904829\n",
            "\n",
            "Epoch [2 / 10]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over train batches: 100%|██████████| 7493/7493 [00:45<00:00, 166.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.22961809910318873\n",
            "\n",
            "Train accuracy: 0.9363832452689604\n",
            "\n",
            "Train precision_micro: 0.9363832452689604\n",
            "\n",
            "Train precision_macro: 0.7589569960594708\n",
            "\n",
            "Train precision_weighted: 0.9196829935311344\n",
            "\n",
            "Train recall_micro: 0.9363832452689604\n",
            "\n",
            "Train recall_macro: 0.7446670652593234\n",
            "\n",
            "Train recall_weighted: 0.9363832452689604\n",
            "\n",
            "Train f1_micro: 0.9363832452689604\n",
            "\n",
            "Train f1_macro: 0.7416181511884303\n",
            "\n",
            "Train f1_weighted: 0.923092098652713\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over test batches: 100%|██████████| 3465/3465 [00:13<00:00, 265.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss:  0.2361616498279128\n",
            "\n",
            "Test accuracy: 0.9354519620684296\n",
            "\n",
            "Test precision_micro: 0.9354519620684296\n",
            "\n",
            "Test precision_macro: 0.807960120240144\n",
            "\n",
            "Test precision_weighted: 0.9242171586517458\n",
            "\n",
            "Test recall_micro: 0.9354519620684296\n",
            "\n",
            "Test recall_macro: 0.80792465442088\n",
            "\n",
            "Test recall_weighted: 0.9354519620684296\n",
            "\n",
            "Test f1_micro: 0.9354519620684296\n",
            "\n",
            "Test f1_macro: 0.8017679543020355\n",
            "\n",
            "Test f1_weighted: 0.9255271898962525\n",
            "\n",
            "Epoch [3 / 10]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over train batches: 100%|██████████| 7493/7493 [00:45<00:00, 166.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.13916931664543486\n",
            "\n",
            "Train accuracy: 0.9633213024424101\n",
            "\n",
            "Train precision_micro: 0.9633213024424101\n",
            "\n",
            "Train precision_macro: 0.8495200115733478\n",
            "\n",
            "Train precision_weighted: 0.9570327096573189\n",
            "\n",
            "Train recall_micro: 0.9633213024424101\n",
            "\n",
            "Train recall_macro: 0.8374389648170892\n",
            "\n",
            "Train recall_weighted: 0.9633213024424101\n",
            "\n",
            "Train f1_micro: 0.9633213024424101\n",
            "\n",
            "Train f1_macro: 0.8364440782006224\n",
            "\n",
            "Train f1_weighted: 0.9572365838061141\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over test batches: 100%|██████████| 3465/3465 [00:13<00:00, 262.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss:  0.1898215619387338\n",
            "\n",
            "Test accuracy: 0.9453469242404784\n",
            "\n",
            "Test precision_micro: 0.9453469242404784\n",
            "\n",
            "Test precision_macro: 0.8390925088715424\n",
            "\n",
            "Test precision_weighted: 0.9397381187025384\n",
            "\n",
            "Test recall_micro: 0.9453469242404784\n",
            "\n",
            "Test recall_macro: 0.8368064354209104\n",
            "\n",
            "Test recall_weighted: 0.9453469242404784\n",
            "\n",
            "Test f1_micro: 0.9453469242404784\n",
            "\n",
            "Test f1_macro: 0.8324088545361629\n",
            "\n",
            "Test f1_weighted: 0.9386422238154776\n",
            "\n",
            "Epoch [4 / 10]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over train batches: 100%|██████████| 7493/7493 [00:45<00:00, 165.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.08828795019920804\n",
            "\n",
            "Train accuracy: 0.9783703213573284\n",
            "\n",
            "Train precision_micro: 0.9783703213573284\n",
            "\n",
            "Train precision_macro: 0.9097497641777784\n",
            "\n",
            "Train precision_weighted: 0.9755774493124129\n",
            "\n",
            "Train recall_micro: 0.9783703213573284\n",
            "\n",
            "Train recall_macro: 0.9020762422457085\n",
            "\n",
            "Train recall_weighted: 0.9783703213573284\n",
            "\n",
            "Train f1_micro: 0.9783703213573284\n",
            "\n",
            "Train f1_macro: 0.9012035422052596\n",
            "\n",
            "Train f1_weighted: 0.9752179951506461\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over test batches: 100%|██████████| 3465/3465 [00:13<00:00, 258.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss:  0.1941227726770792\n",
            "\n",
            "Test accuracy: 0.9472672032928168\n",
            "\n",
            "Test precision_micro: 0.9472672032928168\n",
            "\n",
            "Test precision_macro: 0.8494671082791198\n",
            "\n",
            "Test precision_weighted: 0.9346271142058237\n",
            "\n",
            "Test recall_micro: 0.9472672032928168\n",
            "\n",
            "Test recall_macro: 0.8455338758491252\n",
            "\n",
            "Test recall_weighted: 0.9472672032928168\n",
            "\n",
            "Test f1_micro: 0.9472672032928168\n",
            "\n",
            "Test f1_macro: 0.8425009301100297\n",
            "\n",
            "Test f1_weighted: 0.9373948114341172\n",
            "\n",
            "Epoch [5 / 10]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over train batches: 100%|██████████| 7493/7493 [00:46<00:00, 160.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.054092766817184564\n",
            "\n",
            "Train accuracy: 0.9885305734464097\n",
            "\n",
            "Train precision_micro: 0.9885305734464097\n",
            "\n",
            "Train precision_macro: 0.9491285912690479\n",
            "\n",
            "Train precision_weighted: 0.987803407394622\n",
            "\n",
            "Train recall_micro: 0.9885305734464097\n",
            "\n",
            "Train recall_macro: 0.9438042004056068\n",
            "\n",
            "Train recall_weighted: 0.9885305734464097\n",
            "\n",
            "Train f1_micro: 0.9885305734464097\n",
            "\n",
            "Train f1_macro: 0.9435370199453973\n",
            "\n",
            "Train f1_weighted: 0.9871615595290842\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over test batches: 100%|██████████| 3465/3465 [00:13<00:00, 256.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss:  0.18050228564256474\n",
            "\n",
            "Test accuracy: 0.9507893717083957\n",
            "\n",
            "Test precision_micro: 0.9507893717083957\n",
            "\n",
            "Test precision_macro: 0.8585627667475225\n",
            "\n",
            "Test precision_weighted: 0.9437647051388238\n",
            "\n",
            "Test recall_micro: 0.9507893717083957\n",
            "\n",
            "Test recall_macro: 0.8541539325054438\n",
            "\n",
            "Test recall_weighted: 0.9507893717083957\n",
            "\n",
            "Test f1_micro: 0.9507893717083957\n",
            "\n",
            "Test f1_macro: 0.8516592052233412\n",
            "\n",
            "Test f1_weighted: 0.9441008825880375\n",
            "\n",
            "Epoch [6 / 10]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over train batches: 100%|██████████| 7493/7493 [00:47<00:00, 158.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.03360641317539604\n",
            "\n",
            "Train accuracy: 0.993775193922638\n",
            "\n",
            "Train precision_micro: 0.993775193922638\n",
            "\n",
            "Train precision_macro: 0.971636857534575\n",
            "\n",
            "Train precision_weighted: 0.9934980362880583\n",
            "\n",
            "Train recall_micro: 0.993775193922638\n",
            "\n",
            "Train recall_macro: 0.9688771906282412\n",
            "\n",
            "Train recall_weighted: 0.993775193922638\n",
            "\n",
            "Train f1_micro: 0.993775193922638\n",
            "\n",
            "Train f1_macro: 0.9686071736516356\n",
            "\n",
            "Train f1_weighted: 0.9930952024524213\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over test batches: 100%|██████████| 3465/3465 [00:13<00:00, 260.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss:  0.19222022782592452\n",
            "\n",
            "Test accuracy: 0.9501448869623431\n",
            "\n",
            "Test precision_micro: 0.9501448869623431\n",
            "\n",
            "Test precision_macro: 0.8575963605927024\n",
            "\n",
            "Test precision_weighted: 0.9452415792247478\n",
            "\n",
            "Test recall_micro: 0.9501448869623431\n",
            "\n",
            "Test recall_macro: 0.8547668956347899\n",
            "\n",
            "Test recall_weighted: 0.9501448869623431\n",
            "\n",
            "Test f1_micro: 0.9501448869623431\n",
            "\n",
            "Test f1_macro: 0.8517443738160057\n",
            "\n",
            "Test f1_weighted: 0.9446790748062706\n",
            "\n",
            "Epoch [7 / 10]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over train batches: 100%|██████████| 7493/7493 [00:45<00:00, 164.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.0209585287642798\n",
            "\n",
            "Train accuracy: 0.9966695800996729\n",
            "\n",
            "Train precision_micro: 0.9966695800996729\n",
            "\n",
            "Train precision_macro: 0.9858063575218745\n",
            "\n",
            "Train precision_weighted: 0.996702139888187\n",
            "\n",
            "Train recall_micro: 0.9966695800996729\n",
            "\n",
            "Train recall_macro: 0.9848740189120971\n",
            "\n",
            "Train recall_weighted: 0.9966695800996729\n",
            "\n",
            "Train f1_micro: 0.9966695800996729\n",
            "\n",
            "Train f1_macro: 0.9844741679511118\n",
            "\n",
            "Train f1_weighted: 0.9963815464934842\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over test batches: 100%|██████████| 3465/3465 [00:13<00:00, 262.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss:  0.20301924770012184\n",
            "\n",
            "Test accuracy: 0.9512521623881763\n",
            "\n",
            "Test precision_micro: 0.9512521623881763\n",
            "\n",
            "Test precision_macro: 0.8586707272077418\n",
            "\n",
            "Test precision_weighted: 0.9459883757125684\n",
            "\n",
            "Test recall_micro: 0.9512521623881763\n",
            "\n",
            "Test recall_macro: 0.8561864611731571\n",
            "\n",
            "Test recall_weighted: 0.9512521623881763\n",
            "\n",
            "Test f1_micro: 0.9512521623881763\n",
            "\n",
            "Test f1_macro: 0.8528668946699279\n",
            "\n",
            "Test f1_weighted: 0.9456050771561271\n",
            "\n",
            "Epoch [8 / 10]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over train batches: 100%|██████████| 7493/7493 [00:45<00:00, 164.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.01421172987934263\n",
            "\n",
            "Train accuracy: 0.998052899744552\n",
            "\n",
            "Train precision_micro: 0.998052899744552\n",
            "\n",
            "Train precision_macro: 0.9922665006306491\n",
            "\n",
            "Train precision_weighted: 0.9981200702057011\n",
            "\n",
            "Train recall_micro: 0.998052899744552\n",
            "\n",
            "Train recall_macro: 0.9917596183506235\n",
            "\n",
            "Train recall_weighted: 0.998052899744552\n",
            "\n",
            "Train f1_micro: 0.998052899744552\n",
            "\n",
            "Train f1_macro: 0.991537404241475\n",
            "\n",
            "Train f1_weighted: 0.9979206485610546\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over test batches: 100%|██████████| 3465/3465 [00:13<00:00, 262.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss:  0.21679004889798828\n",
            "\n",
            "Test accuracy: 0.9520400205904608\n",
            "\n",
            "Test precision_micro: 0.9520400205904608\n",
            "\n",
            "Test precision_macro: 0.8621017770121172\n",
            "\n",
            "Test precision_weighted: 0.9435099966529578\n",
            "\n",
            "Test recall_micro: 0.9520400205904608\n",
            "\n",
            "Test recall_macro: 0.8591153850075255\n",
            "\n",
            "Test recall_weighted: 0.9520400205904608\n",
            "\n",
            "Test f1_micro: 0.9520400205904608\n",
            "\n",
            "Test f1_macro: 0.8559924446308155\n",
            "\n",
            "Test f1_weighted: 0.9446672509284909\n",
            "\n",
            "Epoch [9 / 10]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over train batches: 100%|██████████| 7493/7493 [00:44<00:00, 166.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.01041667336779146\n",
            "\n",
            "Train accuracy: 0.9986506729067453\n",
            "\n",
            "Train precision_micro: 0.9986506729067453\n",
            "\n",
            "Train precision_macro: 0.9957840555858732\n",
            "\n",
            "Train precision_weighted: 0.9986706997208601\n",
            "\n",
            "Train recall_micro: 0.9986506729067453\n",
            "\n",
            "Train recall_macro: 0.9956154804323657\n",
            "\n",
            "Train recall_weighted: 0.9986506729067453\n",
            "\n",
            "Train f1_micro: 0.9986506729067453\n",
            "\n",
            "Train f1_macro: 0.9954763820189725\n",
            "\n",
            "Train f1_weighted: 0.9985495371763111\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over test batches: 100%|██████████| 3465/3465 [00:12<00:00, 268.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss:  0.2371936649233919\n",
            "\n",
            "Test accuracy: 0.9497575159772604\n",
            "\n",
            "Test precision_micro: 0.9497575159772604\n",
            "\n",
            "Test precision_macro: 0.859133766710672\n",
            "\n",
            "Test precision_weighted: 0.9455436439365319\n",
            "\n",
            "Test recall_micro: 0.9497575159772604\n",
            "\n",
            "Test recall_macro: 0.8552219786564964\n",
            "\n",
            "Test recall_weighted: 0.9497575159772604\n",
            "\n",
            "Test f1_micro: 0.9497575159772604\n",
            "\n",
            "Test f1_macro: 0.8524032946992061\n",
            "\n",
            "Test f1_weighted: 0.9444836632189634\n",
            "\n",
            "Epoch [10 / 10]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over train batches: 100%|██████████| 7493/7493 [00:44<00:00, 168.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.00740039469562314\n",
            "\n",
            "Train accuracy: 0.9990564916719101\n",
            "\n",
            "Train precision_micro: 0.9990564916719101\n",
            "\n",
            "Train precision_macro: 0.996668776178365\n",
            "\n",
            "Train precision_weighted: 0.9991259905323705\n",
            "\n",
            "Train recall_micro: 0.9990564916719101\n",
            "\n",
            "Train recall_macro: 0.996513996664806\n",
            "\n",
            "Train recall_weighted: 0.9990564916719101\n",
            "\n",
            "Train f1_micro: 0.9990564916719101\n",
            "\n",
            "Train f1_macro: 0.99639246259697\n",
            "\n",
            "Train f1_weighted: 0.9989994346609533\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over test batches: 100%|██████████| 3465/3465 [00:12<00:00, 270.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss:  0.2364178702496697\n",
            "\n",
            "Test accuracy: 0.9503620429829518\n",
            "\n",
            "Test precision_micro: 0.9503620429829518\n",
            "\n",
            "Test precision_macro: 0.8583162953306456\n",
            "\n",
            "Test precision_weighted: 0.9448359967540271\n",
            "\n",
            "Test recall_micro: 0.9503620429829518\n",
            "\n",
            "Test recall_macro: 0.8544482821292358\n",
            "\n",
            "Test recall_weighted: 0.9503620429829518\n",
            "\n",
            "Test f1_micro: 0.9503620429829518\n",
            "\n",
            "Test f1_macro: 0.8519196199888627\n",
            "\n",
            "Test f1_weighted: 0.9446275778191668\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ### START CODE HERE ###\n",
        "train(\n",
        "    n_epochs=10,\n",
        "    model=model,\n",
        "    train_dataloader=train_dataloader,\n",
        "    valid_dataloader=valid_dataloader,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    writer=writer,\n",
        "    device=device,\n",
        "    model_type=f'BiLSTM')\n",
        "# ### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrW7cODysJ4j"
      },
      "source": [
        "Здесь и далее проинициализируем *tensorboard* для логгирования метрики в процессе обучения:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 383,
      "metadata": {
        "id": "bNIk2Rd5kjRa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 29228), started 5:25:22 ago. (Use '!kill 29228' to kill it.)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
              "          const url = new URL(\"http://localhost\");\n",
              "          const port = 6006;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "En2_vpe7jTwG"
      },
      "source": [
        "Проверим качество на тестовой выборке, ожидаем `f1_macro >= 0.76`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 384,
      "metadata": {
        "id": "3-tOLEdbQ6gD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over test batches: 100%|██████████| 3683/3683 [00:13<00:00, 266.99it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss:  0.41061175518721005\n",
            "\n",
            "Test accuracy: 0.9135306402363501\n",
            "\n",
            "Test precision_micro: 0.9135306402363501\n",
            "\n",
            "Test precision_macro: 0.8034701317634443\n",
            "\n",
            "Test precision_weighted: 0.9116306252814053\n",
            "\n",
            "Test recall_micro: 0.9135306402363501\n",
            "\n",
            "Test recall_macro: 0.8010226149589172\n",
            "\n",
            "Test recall_weighted: 0.9135306402363501\n",
            "\n",
            "Test f1_micro: 0.9135306402363501\n",
            "\n",
            "Test f1_macro: 0.7966473742530065\n",
            "\n",
            "Test f1_weighted: 0.9079109149733631\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "evaluate_epoch(\n",
        "    model=model,\n",
        "    dataloader=test_dataloader,\n",
        "    criterion=criterion,\n",
        "    writer=writer,\n",
        "    device=device,\n",
        "    epoch=1,\n",
        "    model_type='BiLSTM'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Мини-отчет \n",
        "* Если увеличить epochs, то модель выдает лучше качество, что видно из графиков. При этом модель получилась достаточно сложной из-за чего она не сильно переобучается.  \n",
        "* Изменение dropout при фиксированном $epochs = 10$ почти не дает прироста (на $\\approx 0.0005$ лучше f1-macro в bilstm с $dropout = 0.1$ чем без него при фиксированном $epochs = 10$). При этом увеличив dropout и сделав равным 0.3 f1-macro дал худший результат среди bilstm c dropout=[0,0.1, 0.3] при $epochs=10$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8R6nopyQEL-"
      },
      "source": [
        "## Часть 3. Transformers-теггер (6 баллов)\n",
        "\n",
        "В данной части задания нужно сделать все то же самое, но с использованием модели на базе архитектуры Transformer, а именно предлагается дообучать предобученную модель **BERT**.\n",
        "\n",
        "Для данной модели подразумевается специальная подготовка данных, с чего мы и начнем:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrbX5gFDQEL-"
      },
      "source": [
        "Модель **BERT** использует специальный токенизатор WordPiece для разбиения предложений на токены. Готовая предобученная версия такого токенизатора существует в библиотеке **transformers**. Есть два класса: `BertTokenizer` и `BertTokenizerFast`. Использовать можно любой, но второй вариант работает существенно быстрее.\n",
        "\n",
        "Токенизаторы можно обучать с нуля на своем корпусе данных, а можно подгружать уже готовые. Готовые токенизаторы, как правило, соответствуют предобученной конфигурации модели, которая использует словарь из этого токенизатора.\n",
        "\n",
        "Мы будем использовать базовую конфигурацию предобученного **BERT** для модели и токенизатора.\n",
        "\n",
        "P.S. Часто приходится проводить эксперименты с моделями разной архитектуры, например **BERT** и **GPT**, поэтому удобно использовать класс `AutoTokenizer`, который по названию модели сам определит, какой класс нужен для инициализации токенизатора.\n",
        "\n",
        "Существует полезный сервис **HuggingFace**, который собрал в себе большое множество моделей и данных, ссылки на ресурс:\n",
        "- Hugging Face: https://huggingface.co\n",
        "- Hugging Face Models: https://huggingface.co/models\n",
        "- Hugging Face Datasets: https://huggingface.co/datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-UTiI4gQEL-"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSbBhvnDQEMA"
      },
      "outputs": [],
      "source": [
        "model_name = \"distilbert-base-cased\"\n",
        "set_global_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxWNX5i6QEMA"
      },
      "source": [
        "Подгружение предобученных моделей и токенизаторов в **huggingface** происходит с помощью конструктора **from_pretrained**.\n",
        "\n",
        "В данном конструкторе можно указать либо путь к предобученному токенизатору, либо название предобученной конфигурации, как в нашем случае: тогда **transformers** сам подгрузит нужные параметры:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tg_bCeaQEMA"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MIrbmNoQEMA"
      },
      "source": [
        "### Подготовка словарей\n",
        "\n",
        "В сравнении с рекуррентными моделями, нам больше не нужно заниматься сборкой словаря, так как это уже сделано заранее благодаря токенизаторам и алгоритмам, стоящими за ними.\n",
        "\n",
        "Но нам как и прежде потребуется:\n",
        "- {**label**}→{**label_idx**}: соответствие между тегом и уникальным индексом (начинается с 0);\n",
        "\n",
        "Но данное отображение у нас уже реализовано в одной из предыдущих частей задания."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvYF-4uaQEMB"
      },
      "source": [
        "### Подготовка датасета и загрузчика\n",
        "\n",
        "Мы также хотим обучать модель батчами, поэтому нам как и прежде понадобятся `Dataset`, `Collator` и `DataLoader`.\n",
        "\n",
        "Но мы не можем переиспользовать те, что в предыдущих частях задания, так как обработка данных должна производится немного иначе с использованием токенизатора.\n",
        "\n",
        "Давайте напишем новый кастомный датасет, который на вход (метод `__init__`) будет принимать:\n",
        "- token_seq - список списков слов / токенов\n",
        "- label_seq - список списков тегов\n",
        "\n",
        "и возвращать из метода `__getitem__` два списка:\n",
        "- список текстовых значений (`List[str]`) из индексов токенов в сэмпле\n",
        "- список целочисленных значений (`List[int]`) из индексов соответвующих тегов\n",
        "\n",
        "P.S. В отличие от предыдущего кастомного датасет, здесь мы возвращаем два `List`'а вместо `torch.LongTensor`, так как логику формирования западдированного батча мы перенесем в `Collator` из-за специфики работы токенизатора - он сам возвращает уже западдированный тензор с индексами токенов, а для индексов тегов нам нужно будет сделать это самостоятельно по аналогии с предыдущим датасетом.\n",
        "\n",
        "**Задание. Реализуйте класс датасета TransformersDataset.** **<font color='red'>(1 балл)</font>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EoNLDOOQEMB"
      },
      "outputs": [],
      "source": [
        "class TransformersDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    Transformers Dataset for NER.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        token_seq: List[List[str]],\n",
        "        label_seq: List[List[str]],\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Class constructor.\n",
        "\n",
        "        Args:\n",
        "            token_seq: the list of lists contains token sequences.\n",
        "            label_seq: the list of lists consists of label sequences.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        self.token_seq = token_seq\n",
        "        self.label_seq = [self.process_labels(labels, label2idx) for labels in label_seq]\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns length of the dataset.\n",
        "\n",
        "        Args:\n",
        "            None\n",
        "\n",
        "        Returns:\n",
        "            length of the dataset\n",
        "        \"\"\"\n",
        "        return len(self.token_seq)\n",
        "\n",
        "    def __getitem__(\n",
        "        self,\n",
        "        idx: int,\n",
        "    ) -> Tuple[List[str], List[int]]:\n",
        "        \"\"\"\n",
        "        Gets one item for tthe dataset\n",
        "\n",
        "        Args:\n",
        "            idx: the index of the particular element in the dataset\n",
        "\n",
        "        Returns:\n",
        "            (tokens, labels), where `tokens` is sequence of token in the dataset\n",
        "                by index `idx` and `labels` is corresponding labels list\n",
        "        \"\"\"\n",
        "        tokens = None\n",
        "        labels = None\n",
        "\n",
        "        # ### START CODE HERE ###\n",
        "        tokens = self.token_seq[idx]\n",
        "        labels = self.label_seq[idx]\n",
        "        # ### END CODE HERE ###\n",
        "\n",
        "        return tokens, labels\n",
        "\n",
        "    @staticmethod\n",
        "    def process_labels(\n",
        "        labels: List[str],\n",
        "        label2idx: Dict[str, int],\n",
        "    ) -> List[int]:\n",
        "        \"\"\"\n",
        "        Transform list of labels into list of labels' indices.\n",
        "\n",
        "        Args:\n",
        "            labels: the list of strings contains the labels\n",
        "            label2idx: mapping from a label to an index\n",
        "\n",
        "        Returns:\n",
        "            ids: the sequence of indices that correspond to labels\n",
        "        \"\"\"\n",
        "\n",
        "        ids = None\n",
        "\n",
        "        # ### START CODE HERE ###\n",
        "        ids = [label2idx[label] for label in labels]\n",
        "        # ### END CODE HERE ###\n",
        "\n",
        "        return ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1oNc-31QEMB"
      },
      "source": [
        "Создадим три датасета:\n",
        "- *train_dataset*\n",
        "- *valid_dataset*\n",
        "- *test_dataset*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqg56Jf8QEMC"
      },
      "outputs": [],
      "source": [
        "train_dataset = TransformersDataset(\n",
        "    token_seq=train_token_seq,\n",
        "    label_seq=train_label_seq,\n",
        ")\n",
        "valid_dataset = TransformersDataset(\n",
        "    token_seq=valid_token_seq,\n",
        "    label_seq=valid_label_seq,\n",
        ")\n",
        "test_dataset = TransformersDataset(\n",
        "    token_seq=test_token_seq,\n",
        "    label_seq=test_label_seq,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdIS6XrvQEMC"
      },
      "source": [
        "Посмотрим на то, что мы получили:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IT00Pjy6QEMC"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.'],\n",
              " [3, 0, 2, 0, 0, 0, 2, 0, 0])"
            ]
          },
          "execution_count": 309,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYal2icQmuD-"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['cricket',\n",
              "  '-',\n",
              "  'leicestershire',\n",
              "  'take',\n",
              "  'over',\n",
              "  'at',\n",
              "  'top',\n",
              "  'after',\n",
              "  'innings',\n",
              "  'victory',\n",
              "  '.'],\n",
              " [0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "execution_count": 310,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "valid_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCXd3FWVmuKe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['soccer',\n",
              "  '-',\n",
              "  'japan',\n",
              "  'get',\n",
              "  'lucky',\n",
              "  'win',\n",
              "  ',',\n",
              "  'china',\n",
              "  'in',\n",
              "  'surprise',\n",
              "  'defeat',\n",
              "  '.'],\n",
              " [0, 0, 1, 0, 0, 0, 0, 4, 0, 0, 0, 0])"
            ]
          },
          "execution_count": 311,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4R605vAnYT9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Тесты пройдены!\n"
          ]
        }
      ],
      "source": [
        "assert len(train_dataset) == 14986, \"Неправильная длина train_dataset\"\n",
        "assert len(valid_dataset) == 3465, \"Неправильная длина valid_dataset\"\n",
        "assert len(test_dataset) == 3683, \"Неправильная длина test_dataset\"\n",
        "\n",
        "assert train_dataset[0][0] == ['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.'], \"Неправильно сформированный train_dataset\"\n",
        "assert train_dataset[0][1] == [3,0,2,0,0,0,2,0,0], \"Неправильно сформированный train_dataset\"\n",
        "\n",
        "assert valid_dataset[0][0] == ['cricket', '-', 'leicestershire', 'take', 'over', 'at', 'top', 'after', 'innings', 'victory', '.'], \"Неправильно сформированный valid_dataset\"\n",
        "assert valid_dataset[0][1] == [0,0,3,0,0,0,0,0,0,0,0], \"Неправильно сформированный valid_dataset\"\n",
        "\n",
        "assert test_dataset[0][0] == ['soccer', '-', 'japan', 'get', 'lucky', 'win', ',', 'china', 'in', 'surprise', 'defeat', '.'], \"Неправильно сформированный test_dataset\"\n",
        "assert test_dataset[0][1] == [0,0,1,0,0,0,0,4,0,0,0,0], \"Неправильно сформированный test_dataset\"\n",
        "\n",
        "print(\"Тесты пройдены!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zP_6iQnQEMC"
      },
      "source": [
        "Реализуем новый `Collator`.\n",
        "\n",
        "Инициализировать коллатор будет 3 аргументами:\n",
        "- токенизатор\n",
        "- параметры токенизатора в виде словаря (затем используем как `**kwargs`)\n",
        "- id спецтокена для последовательностей тегов (значение -1)\n",
        "\n",
        "Метод `__call__` на вход принимает батч, а именно список кортежей того, что нам возвращается из датасета. В нашем случае это список кортежей двух int64 тензоров - `List[Tuple[torch.LongTensor, torch.LongTensor]]`.\n",
        "\n",
        "На выходе мы хотим получить два тензора:\n",
        "- западденные индексы слов / токенов\n",
        "- западденные индексы тегов\n",
        "\n",
        "**Задание. Реализуйте класс коллатора TransformersCollator.** **<font color='red'>(2 балла)</font>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BonAp65jQEMD"
      },
      "outputs": [],
      "source": [
        "from transformers import PreTrainedTokenizer\n",
        "from transformers.tokenization_utils_base import BatchEncoding\n",
        "\n",
        "class TransformersCollator:\n",
        "    \"\"\"\n",
        "    Transformers Collator that handles variable-size sentences.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer: PreTrainedTokenizer,\n",
        "        tokenizer_kwargs: Dict[str, Any],\n",
        "        label_padding_value: int,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        TransformersCollator class constructor.\n",
        "\n",
        "        Args:\n",
        "            tokenizer: the pretrained tokenizer which converts sentence\n",
        "                to tokens.\n",
        "            tokenizer_kwargs: the arguments of the tokenizer\n",
        "            label_padding_value: the padding value for a label\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        self.tokenizer = tokenizer\n",
        "        self.tokenizer_kwargs = tokenizer_kwargs\n",
        "\n",
        "        self.label_padding_value = label_padding_value\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        batch: List[Tuple[List[str], List[int]]],\n",
        "    ) -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
        "        \"\"\"\n",
        "        Calls transformers' collator.\n",
        "\n",
        "        Args:\n",
        "            batch: One batch with sentence and labels.\n",
        "\n",
        "        Returns:\n",
        "            (tokens, labels), where `tokens` is sequence of token\n",
        "                and `labels` is corresponding labels list\n",
        "        \"\"\"\n",
        "        tokens, labels = zip(*batch)\n",
        "\n",
        "        # ### START CODE HERE ###\n",
        "        tokens = self.tokenizer(tokens, **self.tokenizer_kwargs)\n",
        "        labels = TransformersCollator.encode_labels(\n",
        "            tokens, \n",
        "            labels,\n",
        "            label_padding_value=self.label_padding_value)\n",
        "        # ### END CODE HERE ###\n",
        "\n",
        "        tokens.pop(\"offset_mapping\")\n",
        "\n",
        "        return tokens, labels\n",
        "\n",
        "    @staticmethod\n",
        "    def encode_labels(\n",
        "        tokens: BatchEncoding,\n",
        "        labels: List[List[int]],\n",
        "        label_padding_value: int,\n",
        "    ) -> torch.LongTensor:\n",
        "\n",
        "        encoded_labels = []\n",
        "\n",
        "        for doc_labels, doc_offset in zip(labels, tokens.offset_mapping):\n",
        "\n",
        "            doc_enc_labels = np.ones(len(doc_offset), dtype=int) * label_padding_value\n",
        "            arr_offset = np.array(doc_offset)\n",
        "\n",
        "            doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels\n",
        "            encoded_labels.append(doc_enc_labels.tolist())\n",
        "\n",
        "        return torch.LongTensor(encoded_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iC8JkUPnQEMD"
      },
      "outputs": [],
      "source": [
        "tokenizer_kwargs = {\n",
        "    \"is_split_into_words\":    True,\n",
        "    \"return_offsets_mapping\": True,\n",
        "    \"padding\":                True,\n",
        "    \"truncation\":             True,\n",
        "    \"max_length\":             512,\n",
        "    \"return_tensors\":         \"pt\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sCDaxR6QEMD"
      },
      "outputs": [],
      "source": [
        "collator = TransformersCollator(\n",
        "    tokenizer=tokenizer,\n",
        "    tokenizer_kwargs=tokenizer_kwargs,\n",
        "    label_padding_value=-1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eirev0N_QEMD"
      },
      "source": [
        "Теперь всё готово, чтобы задать `DataLoader`'ы:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JDrLC6pQEME"
      },
      "outputs": [],
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        "    collate_fn=collator,\n",
        ")\n",
        "valid_dataloader = torch.utils.data.DataLoader(\n",
        "    valid_dataset,\n",
        "    batch_size=1,  # для корректных замеров метрик оставить batch_size=1\n",
        "    shuffle=False, # для корректных замеров метрик оставить shuffle=False\n",
        "    collate_fn=collator,\n",
        ")\n",
        "test_dataloader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=1,  # для корректных замеров метрик оставить batch_size=1\n",
        "    shuffle=False, # для корректных замеров метрик оставить shuffle=False\n",
        "    collate_fn=collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3zGjEDHQEME"
      },
      "source": [
        "Посмотрим на то, что мы получили:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSWcYEAWQEME"
      },
      "outputs": [],
      "source": [
        "tokens, labels = next(iter(train_dataloader))\n",
        "\n",
        "tokens = tokens.to(device)\n",
        "labels = labels.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTcdU1BlQEME"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,   170,  4626,  1358,   122,  1685,  3287,   121,   102,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0],\n",
              "        [  101,   118,   118,   185, 20473,  9717,   171,  7836,  9615,  4184,\n",
              "           117,  1821,  4648, 10775,  2371,  6077,  1955,  1406,  1851,  1527,\n",
              "         13837,   102]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
              "       device='cuda:0')}"
            ]
          },
          "execution_count": 318,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7ZTh97-QEME"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-1,  3, -1, -1,  0,  3,  7,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
              "         -1, -1, -1, -1],\n",
              "        [-1,  0, -1,  4, -1, -1,  8, -1, -1, -1,  0,  1, -1, -1,  0, -1,  0,  0,\n",
              "          0, -1,  0, -1]], device='cuda:0')"
            ]
          },
          "execution_count": 319,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMprtk9bodM9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Тесты пройдены!\n"
          ]
        }
      ],
      "source": [
        "train_tokens, train_labels = next(iter(\n",
        "    torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=2,\n",
        "        shuffle=False,\n",
        "        collate_fn=collator,\n",
        "    )\n",
        "))\n",
        "assert torch.equal(train_tokens['input_ids'], torch.tensor([[  101,   174,  1358, 22961,   176, 14170,  1840,  1106, 21423,  9304, 10721,  1324,  2495, 12913,   119,   102], [  101, 11109,  1200,  1602,  6715,   102,     0,     0,     0,     0,    0,     0,     0,     0,     0,     0]])), \"Похоже на ошибку в коллаторе\"\n",
        "assert torch.equal(train_tokens['attention_mask'], torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])), \"Похоже на ошибку в коллаторе\"\n",
        "assert torch.equal(train_labels, torch.tensor([[-1,  3, -1,  0,  2, -1,  0,  0,  0,  2, -1, -1,  0, -1,  0, -1], [-1,  4, -1,  8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]])), \"Похоже на ошибку в коллаторе\"\n",
        "\n",
        "valid_tokens, valid_labels = next(iter(\n",
        "    torch.utils.data.DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=2,\n",
        "        shuffle=False,\n",
        "        collate_fn=collator,\n",
        "    )\n",
        "))\n",
        "assert torch.equal(valid_tokens['input_ids'], torch.tensor([[  101,  5428,   118,  5837, 18117,  5759, 15189,  1321,  1166,  1120,  1499,  1170,  6687,  2681,   119,   102], [  101, 25338, 17996,  1820,   118,  4775,   118,  1476,   102,     0,     0,     0,     0,     0,     0,     0]])), \"Похоже на ошибку в коллаторе\"\n",
        "assert torch.equal(valid_tokens['attention_mask'], torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])), \"Похоже на ошибку в коллаторе\"\n",
        "assert torch.equal(valid_labels, torch.tensor([[-1,  0,  0,  3, -1, -1, -1,  0,  0,  0,  0,  0,  0,  0,  0, -1], [-1,  1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]])), \"Похоже на ошибку в коллаторе\"\n",
        "\n",
        "test_tokens, test_labels = next(iter(\n",
        "    torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=2,\n",
        "        shuffle=False,\n",
        "        collate_fn=collator,\n",
        "    )\n",
        "))\n",
        "assert torch.equal(test_tokens['input_ids'], torch.tensor([[  101,  5862,   118,   179, 26519,  1179,  1243,  6918,  1782,   117,  5144,  1161,  1107,  3774,  3326,   119,   102], [  101,  9468,  3309,  1306, 19122,  2293,   102,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])), \"Похоже на ошибку в коллаторе\"\n",
        "assert torch.equal(test_tokens['attention_mask'], torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])), \"Похоже на ошибку в коллаторе\"\n",
        "assert torch.equal(test_labels, torch.tensor([[-1,  0,  0,  1, -1, -1,  0,  0,  0,  0,  4, -1,  0,  0,  0,  0, -1], [-1,  4, -1, -1,  8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]])), \"Похоже на ошибку в коллаторе\"\n",
        "\n",
        "print(\"Тесты пройдены!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_m-taH0SQEMF"
      },
      "source": [
        "В библиотеке **transformers** есть классы для модели BERT, уже настроенные под решение конкретных задач, с соответствующими головами классификации. Для задачи NER будем использовать класс `BertForTokenClassification`.\n",
        "\n",
        "По аналогии с токенизаторами, мы можем использовать класс `AutoModelForTokenClassification`, который по названию модели сам определит, какой класс нужен для инициализации модели."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6tq_i7JQEMF"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForTokenClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vma9yj0zQEMF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=len(label2idx),\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Imv-6gAQQEMG"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAdHfn4oQEMG"
      },
      "outputs": [],
      "source": [
        "outputs = model(**tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-kTke_8QEMG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Тесты пройдены!\n"
          ]
        }
      ],
      "source": [
        "assert 2 < criterion(outputs[\"logits\"].transpose(1, 2), labels) < 3\n",
        "\n",
        "print(\"Тесты пройдены!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ana4qGKeHrN"
      },
      "outputs": [],
      "source": [
        "# создадим SummaryWriter для эксперимента с BiLSTMModel\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "writer = SummaryWriter(log_dir=f\"logs/Transformer/epochs5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sNuFPRdQEMH"
      },
      "source": [
        "### Эксперименты\n",
        "\n",
        "Проведите эксперименты на данных. Настраивайте параметры по валидационной выборке, не используя тестовую. Ваше цель — настроить сеть так, чтобы качество модели по F1-macro мере на валидационной и тестовой выборках было не меньше **0.9**.\n",
        "\n",
        "Сделайте выводы о качестве модели, переобучении, чувствительности архитектуры к выбору гиперпараметров. Оформите результаты экспериментов в виде мини-отчета (в этом же ipython notebook)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IfkN20lrN0J"
      },
      "source": [
        "Вы можете использовать ту же самую функцию train, что и до этого за тем исключением, что вместо инференса `model(tokens)` нужно делать `model(**tokens)`, а вместо `outputs` использовать `outputs[\"logits\"].transpose(1, 2)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iyZUFddzYE5"
      },
      "source": [
        "**Задание. Проведите эксперименты.** **<font color='red'>(2 балла)</font>**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFW92sNL4YCA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1 / 5]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over train batches: 100%|██████████| 7493/7493 [02:14<00:00, 55.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.07711194621626935\n",
            "\n",
            "Train accuracy: 0.9834108288725469\n",
            "\n",
            "Train precision_micro: 0.9834108288725469\n",
            "\n",
            "Train precision_macro: 0.9238694853184206\n",
            "\n",
            "Train precision_weighted: 0.9845922259864288\n",
            "\n",
            "Train recall_micro: 0.9834108288725469\n",
            "\n",
            "Train recall_macro: 0.9239154740216107\n",
            "\n",
            "Train recall_weighted: 0.9834108288725469\n",
            "\n",
            "Train f1_micro: 0.9834108288725469\n",
            "\n",
            "Train f1_macro: 0.9206051270297984\n",
            "\n",
            "Train f1_weighted: 0.9827186171674714\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over test batches: 100%|██████████| 3465/3465 [00:16<00:00, 209.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss:  0.08454107265112855\n",
            "\n",
            "Test accuracy: 0.9788353049695598\n",
            "\n",
            "Test precision_micro: 0.9788353049695598\n",
            "\n",
            "Test precision_macro: 0.9284025461942818\n",
            "\n",
            "Test precision_weighted: 0.9754100551993752\n",
            "\n",
            "Test recall_micro: 0.9788353049695598\n",
            "\n",
            "Test recall_macro: 0.9279584329217282\n",
            "\n",
            "Test recall_weighted: 0.9788353049695598\n",
            "\n",
            "Test f1_micro: 0.9788353049695598\n",
            "\n",
            "Test f1_macro: 0.9256260059799224\n",
            "\n",
            "Test f1_weighted: 0.9756057833683595\n",
            "\n",
            "Epoch [2 / 5]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over train batches: 100%|██████████| 7493/7493 [02:13<00:00, 56.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.037066608366850294\n",
            "\n",
            "Train accuracy: 0.9934609174450855\n",
            "\n",
            "Train precision_micro: 0.9934609174450855\n",
            "\n",
            "Train precision_macro: 0.9679359939612117\n",
            "\n",
            "Train precision_weighted: 0.9945406016358599\n",
            "\n",
            "Train recall_micro: 0.9934609174450855\n",
            "\n",
            "Train recall_macro: 0.9677724085497732\n",
            "\n",
            "Train recall_weighted: 0.9934609174450855\n",
            "\n",
            "Train f1_micro: 0.9934609174450855\n",
            "\n",
            "Train f1_macro: 0.9663111060688739\n",
            "\n",
            "Train f1_weighted: 0.9934870388960415\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over test batches: 100%|██████████| 3465/3465 [00:16<00:00, 211.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss:  0.07053994417112984\n",
            "\n",
            "Test accuracy: 0.9826172632892702\n",
            "\n",
            "Test precision_micro: 0.9826172632892702\n",
            "\n",
            "Test precision_macro: 0.9442113578051013\n",
            "\n",
            "Test precision_weighted: 0.9840587337398145\n",
            "\n",
            "Test recall_micro: 0.9826172632892702\n",
            "\n",
            "Test recall_macro: 0.9440508637919391\n",
            "\n",
            "Test recall_weighted: 0.9826172632892702\n",
            "\n",
            "Test f1_micro: 0.9826172632892702\n",
            "\n",
            "Test f1_macro: 0.9420565604560562\n",
            "\n",
            "Test f1_weighted: 0.9821337177686575\n",
            "\n",
            "Epoch [3 / 5]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over train batches: 100%|██████████| 7493/7493 [02:13<00:00, 56.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.022950332293910875\n",
            "\n",
            "Train accuracy: 0.996914473194064\n",
            "\n",
            "Train precision_micro: 0.996914473194064\n",
            "\n",
            "Train precision_macro: 0.9850173599454133\n",
            "\n",
            "Train precision_weighted: 0.9974643103186472\n",
            "\n",
            "Train recall_micro: 0.996914473194064\n",
            "\n",
            "Train recall_macro: 0.9852900774126332\n",
            "\n",
            "Train recall_weighted: 0.996914473194064\n",
            "\n",
            "Train f1_micro: 0.996914473194064\n",
            "\n",
            "Train f1_macro: 0.98438821475734\n",
            "\n",
            "Train f1_weighted: 0.9969435303618303\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over test batches: 100%|██████████| 3465/3465 [00:16<00:00, 212.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss:  0.06967636336026341\n",
            "\n",
            "Test accuracy: 0.9837561255379642\n",
            "\n",
            "Test precision_micro: 0.9837561255379642\n",
            "\n",
            "Test precision_macro: 0.9449440286679086\n",
            "\n",
            "Test precision_weighted: 0.9849486289540886\n",
            "\n",
            "Test recall_micro: 0.9837561255379642\n",
            "\n",
            "Test recall_macro: 0.9442247250542928\n",
            "\n",
            "Test recall_weighted: 0.9837561255379642\n",
            "\n",
            "Test f1_micro: 0.9837561255379642\n",
            "\n",
            "Test f1_macro: 0.9425152910068088\n",
            "\n",
            "Test f1_weighted: 0.9831552651624116\n",
            "\n",
            "Epoch [4 / 5]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over train batches: 100%|██████████| 7493/7493 [02:15<00:00, 55.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.01468539714197793\n",
            "\n",
            "Train accuracy: 0.9982027267079157\n",
            "\n",
            "Train precision_micro: 0.9982027267079157\n",
            "\n",
            "Train precision_macro: 0.9908704699528876\n",
            "\n",
            "Train precision_weighted: 0.9985404532016684\n",
            "\n",
            "Train recall_micro: 0.9982027267079157\n",
            "\n",
            "Train recall_macro: 0.9908335873371902\n",
            "\n",
            "Train recall_weighted: 0.9982027267079157\n",
            "\n",
            "Train f1_micro: 0.9982027267079157\n",
            "\n",
            "Train f1_macro: 0.9904232181072536\n",
            "\n",
            "Train f1_weighted: 0.9982268198382609\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over test batches: 100%|██████████| 3465/3465 [00:16<00:00, 208.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss:  0.0758522171478914\n",
            "\n",
            "Test accuracy: 0.9837604283630584\n",
            "\n",
            "Test precision_micro: 0.9837604283630584\n",
            "\n",
            "Test precision_macro: 0.9477510226012996\n",
            "\n",
            "Test precision_weighted: 0.9842714977220052\n",
            "\n",
            "Test recall_micro: 0.9837604283630584\n",
            "\n",
            "Test recall_macro: 0.9455948901484562\n",
            "\n",
            "Test recall_weighted: 0.9837604283630584\n",
            "\n",
            "Test f1_micro: 0.9837604283630584\n",
            "\n",
            "Test f1_macro: 0.944464040474889\n",
            "\n",
            "Test f1_weighted: 0.982742482217624\n",
            "\n",
            "Epoch [5 / 5]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over train batches: 100%|██████████| 7493/7493 [02:14<00:00, 55.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.010239069498965064\n",
            "\n",
            "Train accuracy: 0.9988923753593353\n",
            "\n",
            "Train precision_micro: 0.9988923753593353\n",
            "\n",
            "Train precision_macro: 0.9945021364074781\n",
            "\n",
            "Train precision_weighted: 0.9990431739432906\n",
            "\n",
            "Train recall_micro: 0.9988923753593353\n",
            "\n",
            "Train recall_macro: 0.9946649404614567\n",
            "\n",
            "Train recall_weighted: 0.9988923753593353\n",
            "\n",
            "Train f1_micro: 0.9988923753593353\n",
            "\n",
            "Train f1_macro: 0.9942915029614888\n",
            "\n",
            "Train f1_weighted: 0.9988800638695413\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over test batches: 100%|██████████| 3465/3465 [00:16<00:00, 210.82it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss:  0.08052185976337235\n",
            "\n",
            "Test accuracy: 0.9841246371029222\n",
            "\n",
            "Test precision_micro: 0.9841246371029222\n",
            "\n",
            "Test precision_macro: 0.9452330337433063\n",
            "\n",
            "Test precision_weighted: 0.9865060565186535\n",
            "\n",
            "Test recall_micro: 0.9841246371029222\n",
            "\n",
            "Test recall_macro: 0.9450141181331638\n",
            "\n",
            "Test recall_weighted: 0.9841246371029222\n",
            "\n",
            "Test f1_micro: 0.9841246371029222\n",
            "\n",
            "Test f1_macro: 0.943025921729523\n",
            "\n",
            "Test f1_weighted: 0.98414036639468\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ### START CODE HERE ###\n",
        "# Реализуйте ветку elif в функции train, которая\n",
        "# отвечает условию model_type == 'Transformer'\n",
        "# ### START CODE HERE ###\n",
        "train(\n",
        "    n_epochs=5,\n",
        "    model=model,\n",
        "    train_dataloader=train_dataloader,\n",
        "    valid_dataloader=valid_dataloader,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    writer=writer,\n",
        "    device=device,\n",
        "    model_type='Transformer')\n",
        "# ### END CODE HERE ###)\n",
        "# ### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad4qt1dy6NuD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over test batches: 100%|██████████| 3683/3683 [00:17<00:00, 209.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss:  0.1684116714474353\n",
            "\n",
            "Test accuracy: 0.965868280906624\n",
            "\n",
            "Test precision_micro: 0.965868280906624\n",
            "\n",
            "Test precision_macro: 0.9158423414274492\n",
            "\n",
            "Test precision_weighted: 0.9664471804199872\n",
            "\n",
            "Test recall_micro: 0.965868280906624\n",
            "\n",
            "Test recall_macro: 0.9154741257545571\n",
            "\n",
            "Test recall_weighted: 0.965868280906624\n",
            "\n",
            "Test f1_micro: 0.965868280906624\n",
            "\n",
            "Test f1_macro: 0.9139659931625912\n",
            "\n",
            "Test f1_weighted: 0.9649466209220546\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "evaluate_epoch(\n",
        "    model=model,\n",
        "    dataloader=test_dataloader,\n",
        "    criterion=criterion,\n",
        "    writer=writer,\n",
        "    device=device,\n",
        "    epoch=1,\n",
        "    model_type='Transformer',\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Мини-отчет\n",
        "Качество получилось выше чем у BiLSTM, при этом с увеличением эпох есть прирост в качестве"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XlI3cb1QEL2"
      },
      "source": [
        "## Часть 4. Бонусы.\n",
        "\n",
        "## Бонус 1: BiLSTMAttention-теггер (2 баллa)\n",
        "\n",
        "Необходимо провести те же самые эксперименты как и в части 2, но уже с использованием усовершенствованной архитектуры теггера BiLSTM с Attention механизмом.\n",
        "\n",
        "**Обратите внимание**, что реализовывать Attention самому не нужно, можно использовать `torch.nn.MultiheadAttention`.\n",
        "\n",
        "Также сделайте выводы о качестве модели, переобучении, чувствительности архитектуры к выбору гиперпараметров и проведите небольшой сравнительный анализ с предыдущей архитектурой. Оформите результаты экспериментов в виде мини-отчета (в этом же ipython notebook).\n",
        "\n",
        "**Задание. Реализуйте класс модели BiLSTMAttn.** **<font color='red'>(1 балл)</font>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 391,
      "metadata": {
        "id": "_MyLQp047yID"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Embedding, LSTM, Linear, MultiheadAttention\n",
        "\n",
        "\n",
        "class BiLSTMAttn(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Bidirectional LSTM architecture.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_embeddings: int,\n",
        "        embedding_dim: int,\n",
        "        hidden_size: int,\n",
        "        num_layers: int,\n",
        "        dropout: float,\n",
        "        bidirectional: bool,\n",
        "        n_classes: int,\n",
        "        token_padding_value: int,\n",
        "        max_norm: float,\n",
        "        num_heads: int,\n",
        "    ) -> None:\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_padding_value = token_padding_value\n",
        "        self.embedding = None # Embedding layer\n",
        "        self.rnn = None # LSTM layer\n",
        "        self.q_linear = None # rnn output to q (queries)\n",
        "        self.k_linear = None # rnn output to k (keys)\n",
        "        self.v_linear = None # rnn output to v 9values)\n",
        "        self.multihead_attn = None # MultiHead Attention\n",
        "        self.head = None # Linear layer\n",
        "\n",
        "        # ### START CODE HERE ###\n",
        "        self.embedding = torch.nn.Embedding(num_embeddings, embedding_dim)\n",
        "        self.rnn = torch.nn.LSTM(\n",
        "            embedding_dim,\n",
        "            hidden_size,\n",
        "            num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "            bidirectional=bidirectional)\n",
        "        \n",
        "        self.q_linear = torch.nn.Linear(hidden_size * (bidirectional + 1),\n",
        "                                        hidden_size * (bidirectional + 1))\n",
        "        \n",
        "        self.k_linear = torch.nn.Linear(hidden_size * (bidirectional + 1), \n",
        "                                        hidden_size * (bidirectional + 1))\n",
        "        \n",
        "        self.v_linear = torch.nn.Linear(hidden_size * (bidirectional + 1), \n",
        "                                        hidden_size * (bidirectional + 1))\n",
        "        \n",
        "        self.multihead_attn = torch.nn.MultiheadAttention(\n",
        "            hidden_size * (bidirectional + 1),\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            batch_first=True)\n",
        "        \n",
        "        self.head = torch.nn.Linear(hidden_size * (bidirectional + 1), n_classes)\n",
        "        # ### END CODE HERE ###\n",
        "\n",
        "    def forward(self, tokens: torch.LongTensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Applying neural network layers to input 'tokens'.\n",
        "\n",
        "        Args:\n",
        "            tokens: the input tensor with tokens ids (batch_size, sequence_len)\n",
        "\n",
        "        Returns:\n",
        "            logits: the scores issued by the model (batch_size, num_classes, sequence_len)\n",
        "        \"\"\"\n",
        "        embed = self.embedding(tokens)\n",
        "\n",
        "        # Используем специальную функцию pack_padded_sequence для того, чтобы получить\n",
        "        # структуру PackedSequence, которая не учитывать паддинг при проходе rnn.\n",
        "        # lengths -- длины исходных исходных последовательностей в батче,\n",
        "        # без учёта сдвига\n",
        "        lengths = (tokens != self.token_padding_value).sum(dim=1).detach().cpu()\n",
        "        packed_embed = torch.nn.utils.rnn.pack_padded_sequence(\n",
        "            input=embed,\n",
        "            lengths=lengths,\n",
        "            batch_first=True,\n",
        "            enforce_sorted=False,\n",
        "        )\n",
        "\n",
        "        # Используем специальную функцию pad_packed_sequence для того, чтобы получить\n",
        "        # тензор из PackedSequence. Операция является обратной к pack_padded_sequence\n",
        "        packed_rnn_output, _ = self.rnn(packed_embed)\n",
        "        rnn_output, _ = torch.nn.utils.rnn.pad_packed_sequence(\n",
        "            sequence=packed_rnn_output,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # ### START CODE HERE ###\n",
        "        query = self.q_linear(rnn_output)\n",
        "        key = self.k_linear(rnn_output)\n",
        "        value = self.v_linear(rnn_output)\n",
        "\n",
        "        key_padding_mask = tokens == self.token_padding_value\n",
        "        attn_output, _ = self.multihead_attn(query, key, value, key_padding_mask)\n",
        "        # ### END CODE HERE ###\n",
        "\n",
        "        logits = self.head(attn_output)\n",
        "        logits = logits.transpose(1, 2)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezh9kLTkQEL9"
      },
      "source": [
        "**Задание. Проведите эксперименты и побейте метрику из части 2.** **<font color='red'>(1 балл)</font>**\n",
        "\n",
        "P.S. Eсли качества увеличить не получилось, это нужно обосновать"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 400,
      "metadata": {
        "id": "sE1C1tzEQEL-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1 / 10]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over train batches: 100%|██████████| 7493/7493 [00:52<00:00, 143.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.5585736999305665\n",
            "\n",
            "Train accuracy: 0.8468972139972784\n",
            "\n",
            "Train precision_micro: 0.8468972139972784\n",
            "\n",
            "Train precision_macro: 0.4841463988886224\n",
            "\n",
            "Train precision_weighted: 0.7734003099878388\n",
            "\n",
            "Train recall_micro: 0.8468972139972784\n",
            "\n",
            "Train recall_macro: 0.5203094591290742\n",
            "\n",
            "Train recall_weighted: 0.8468972139972784\n",
            "\n",
            "Train f1_micro: 0.8468972139972784\n",
            "\n",
            "Train f1_macro: 0.49184254217880874\n",
            "\n",
            "Train f1_weighted: 0.8010710009617817\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over test batches: 100%|██████████| 3465/3465 [00:13<00:00, 251.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss:  0.3711425573336351\n",
            "\n",
            "Test accuracy: 0.8953347832428138\n",
            "\n",
            "Test precision_micro: 0.8953347832428138\n",
            "\n",
            "Test precision_macro: 0.6982127273063949\n",
            "\n",
            "Test precision_weighted: 0.8654964579414758\n",
            "\n",
            "Test recall_micro: 0.8953347832428138\n",
            "\n",
            "Test recall_macro: 0.7172847432955289\n",
            "\n",
            "Test recall_weighted: 0.8953347832428138\n",
            "\n",
            "Test f1_micro: 0.8953347832428138\n",
            "\n",
            "Test f1_macro: 0.6999164348412874\n",
            "\n",
            "Test f1_weighted: 0.8741971025153257\n",
            "\n",
            "Epoch [2 / 10]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over train batches: 100%|██████████| 7493/7493 [00:51<00:00, 145.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.2426574010019167\n",
            "\n",
            "Train accuracy: 0.9374805371543612\n",
            "\n",
            "Train precision_micro: 0.9374805371543612\n",
            "\n",
            "Train precision_macro: 0.7502468573710664\n",
            "\n",
            "Train precision_weighted: 0.9251408512047681\n",
            "\n",
            "Train recall_micro: 0.9374805371543612\n",
            "\n",
            "Train recall_macro: 0.7479393054564795\n",
            "\n",
            "Train recall_weighted: 0.9374805371543612\n",
            "\n",
            "Train f1_micro: 0.9374805371543612\n",
            "\n",
            "Train f1_macro: 0.7400480615526261\n",
            "\n",
            "Train f1_weighted: 0.9269914604429583\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over test batches: 100%|██████████| 3465/3465 [00:13<00:00, 251.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss:  0.287193515466397\n",
            "\n",
            "Test accuracy: 0.9196638658025641\n",
            "\n",
            "Test precision_micro: 0.9196638658025641\n",
            "\n",
            "Test precision_macro: 0.7667103290652664\n",
            "\n",
            "Test precision_weighted: 0.9012811410329196\n",
            "\n",
            "Test recall_micro: 0.9196638658025641\n",
            "\n",
            "Test recall_macro: 0.7716188027379972\n",
            "\n",
            "Test recall_weighted: 0.9196638658025641\n",
            "\n",
            "Test f1_micro: 0.9196638658025641\n",
            "\n",
            "Test f1_macro: 0.7626836546676322\n",
            "\n",
            "Test f1_weighted: 0.9055675715435968\n",
            "\n",
            "Epoch [3 / 10]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over train batches: 100%|██████████| 7493/7493 [00:51<00:00, 144.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.1375646395973824\n",
            "\n",
            "Train accuracy: 0.9674975804849727\n",
            "\n",
            "Train precision_micro: 0.9674975804849727\n",
            "\n",
            "Train precision_macro: 0.8504565697720515\n",
            "\n",
            "Train precision_weighted: 0.9635450699007023\n",
            "\n",
            "Train recall_micro: 0.9674975804849727\n",
            "\n",
            "Train recall_macro: 0.8463768644363416\n",
            "\n",
            "Train recall_weighted: 0.9674975804849727\n",
            "\n",
            "Train f1_micro: 0.9674975804849727\n",
            "\n",
            "Train f1_macro: 0.8421863910523688\n",
            "\n",
            "Train f1_weighted: 0.9631795699937554\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over test batches: 100%|██████████| 3465/3465 [00:13<00:00, 252.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss:  0.2769134621195899\n",
            "\n",
            "Test accuracy: 0.9274091970073626\n",
            "\n",
            "Test precision_micro: 0.9274091970073626\n",
            "\n",
            "Test precision_macro: 0.7907376536978317\n",
            "\n",
            "Test precision_weighted: 0.9110810268041942\n",
            "\n",
            "Test recall_micro: 0.9274091970073626\n",
            "\n",
            "Test recall_macro: 0.7940916506287304\n",
            "\n",
            "Test recall_weighted: 0.9274091970073626\n",
            "\n",
            "Test f1_micro: 0.9274091970073626\n",
            "\n",
            "Test f1_macro: 0.7858284090142214\n",
            "\n",
            "Test f1_weighted: 0.9142728229867073\n",
            "\n",
            "Epoch [4 / 10]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over train batches: 100%|██████████| 7493/7493 [00:52<00:00, 143.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.09562103301202365\n",
            "\n",
            "Train accuracy: 0.9793889778392274\n",
            "\n",
            "Train precision_micro: 0.9793889778392274\n",
            "\n",
            "Train precision_macro: 0.901431810239436\n",
            "\n",
            "Train precision_weighted: 0.9773036449282865\n",
            "\n",
            "Train recall_micro: 0.9793889778392274\n",
            "\n",
            "Train recall_macro: 0.8968218929173319\n",
            "\n",
            "Train recall_weighted: 0.9793889778392274\n",
            "\n",
            "Train f1_micro: 0.9793889778392274\n",
            "\n",
            "Train f1_macro: 0.894613661818728\n",
            "\n",
            "Train f1_weighted: 0.9767417000424692\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over test batches: 100%|██████████| 3465/3465 [00:13<00:00, 252.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss:  0.273246463275892\n",
            "\n",
            "Test accuracy: 0.9327351081973888\n",
            "\n",
            "Test precision_micro: 0.9327351081973888\n",
            "\n",
            "Test precision_macro: 0.8117058228502267\n",
            "\n",
            "Test precision_weighted: 0.9168849680710375\n",
            "\n",
            "Test recall_micro: 0.9327351081973888\n",
            "\n",
            "Test recall_macro: 0.8132167384246223\n",
            "\n",
            "Test recall_weighted: 0.9327351081973888\n",
            "\n",
            "Test f1_micro: 0.9327351081973888\n",
            "\n",
            "Test f1_macro: 0.80647472573989\n",
            "\n",
            "Test f1_weighted: 0.9202821258931821\n",
            "\n",
            "Epoch [5 / 10]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over train batches: 100%|██████████| 7493/7493 [00:51<00:00, 145.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.07029857384347568\n",
            "\n",
            "Train accuracy: 0.9852566653651299\n",
            "\n",
            "Train precision_micro: 0.9852566653651299\n",
            "\n",
            "Train precision_macro: 0.9294484845459169\n",
            "\n",
            "Train precision_weighted: 0.9843463729395742\n",
            "\n",
            "Train recall_micro: 0.9852566653651299\n",
            "\n",
            "Train recall_macro: 0.9270633575521521\n",
            "\n",
            "Train recall_weighted: 0.9852566653651299\n",
            "\n",
            "Train f1_micro: 0.9852566653651299\n",
            "\n",
            "Train f1_macro: 0.9246608691019464\n",
            "\n",
            "Train f1_weighted: 0.9835889653828862\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over test batches: 100%|██████████| 3465/3465 [00:13<00:00, 255.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss:  0.2664192044151335\n",
            "\n",
            "Test accuracy: 0.9352275375486405\n",
            "\n",
            "Test precision_micro: 0.9352275375486405\n",
            "\n",
            "Test precision_macro: 0.818821894187851\n",
            "\n",
            "Test precision_weighted: 0.9284452383320131\n",
            "\n",
            "Test recall_micro: 0.9352275375486405\n",
            "\n",
            "Test recall_macro: 0.8190884384683571\n",
            "\n",
            "Test recall_weighted: 0.9352275375486405\n",
            "\n",
            "Test f1_micro: 0.9352275375486405\n",
            "\n",
            "Test f1_macro: 0.813221109249951\n",
            "\n",
            "Test f1_weighted: 0.9276654593933757\n",
            "\n",
            "Epoch [6 / 10]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over train batches: 100%|██████████| 7493/7493 [00:51<00:00, 145.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.05596827645813925\n",
            "\n",
            "Train accuracy: 0.9893272146947154\n",
            "\n",
            "Train precision_micro: 0.9893272146947154\n",
            "\n",
            "Train precision_macro: 0.949350613066759\n",
            "\n",
            "Train precision_weighted: 0.9890668341795582\n",
            "\n",
            "Train recall_micro: 0.9893272146947154\n",
            "\n",
            "Train recall_macro: 0.9466369510711765\n",
            "\n",
            "Train recall_weighted: 0.9893272146947154\n",
            "\n",
            "Train f1_micro: 0.9893272146947154\n",
            "\n",
            "Train f1_macro: 0.9452107921406957\n",
            "\n",
            "Train f1_weighted: 0.9882933436915132\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over test batches: 100%|██████████| 3465/3465 [00:13<00:00, 261.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss:  0.28178685296822487\n",
            "\n",
            "Test accuracy: 0.9394591416714065\n",
            "\n",
            "Test precision_micro: 0.9394591416714065\n",
            "\n",
            "Test precision_macro: 0.8278383462742264\n",
            "\n",
            "Test precision_weighted: 0.9308344016314533\n",
            "\n",
            "Test recall_micro: 0.9394591416714065\n",
            "\n",
            "Test recall_macro: 0.8260594198097966\n",
            "\n",
            "Test recall_weighted: 0.9394591416714065\n",
            "\n",
            "Test f1_micro: 0.9394591416714065\n",
            "\n",
            "Test f1_macro: 0.8215238804458281\n",
            "\n",
            "Test f1_weighted: 0.9312758331799671\n",
            "\n",
            "Epoch [7 / 10]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over train batches: 100%|██████████| 7493/7493 [00:50<00:00, 147.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.04596590870948358\n",
            "\n",
            "Train accuracy: 0.991558239028944\n",
            "\n",
            "Train precision_micro: 0.991558239028944\n",
            "\n",
            "Train precision_macro: 0.9597292688889947\n",
            "\n",
            "Train precision_weighted: 0.991345917552965\n",
            "\n",
            "Train recall_micro: 0.991558239028944\n",
            "\n",
            "Train recall_macro: 0.9575348558043353\n",
            "\n",
            "Train recall_weighted: 0.991558239028944\n",
            "\n",
            "Train f1_micro: 0.991558239028944\n",
            "\n",
            "Train f1_macro: 0.9563088752550394\n",
            "\n",
            "Train f1_weighted: 0.9907361331816669\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over test batches: 100%|██████████| 3465/3465 [00:13<00:00, 256.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss:  0.2762411178351454\n",
            "\n",
            "Test accuracy: 0.9410390071749588\n",
            "\n",
            "Test precision_micro: 0.9410390071749588\n",
            "\n",
            "Test precision_macro: 0.8336581245326796\n",
            "\n",
            "Test precision_weighted: 0.9324183786174478\n",
            "\n",
            "Test recall_micro: 0.9410390071749588\n",
            "\n",
            "Test recall_macro: 0.8326485768434654\n",
            "\n",
            "Test recall_weighted: 0.9410390071749588\n",
            "\n",
            "Test f1_micro: 0.9410390071749588\n",
            "\n",
            "Test f1_macro: 0.8277915557641411\n",
            "\n",
            "Test f1_weighted: 0.932917699177676\n",
            "\n",
            "Epoch [8 / 10]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over train batches: 100%|██████████| 7493/7493 [00:51<00:00, 146.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.03678841702174405\n",
            "\n",
            "Train accuracy: 0.9934371355466379\n",
            "\n",
            "Train precision_micro: 0.9934371355466379\n",
            "\n",
            "Train precision_macro: 0.9687712925147113\n",
            "\n",
            "Train precision_weighted: 0.9932213835220115\n",
            "\n",
            "Train recall_micro: 0.9934371355466379\n",
            "\n",
            "Train recall_macro: 0.9673343519288868\n",
            "\n",
            "Train recall_weighted: 0.9934371355466379\n",
            "\n",
            "Train f1_micro: 0.9934371355466379\n",
            "\n",
            "Train f1_macro: 0.966346760745071\n",
            "\n",
            "Train f1_weighted: 0.9927940965347872\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over test batches: 100%|██████████| 3465/3465 [00:13<00:00, 265.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss:  0.29747675220495073\n",
            "\n",
            "Test accuracy: 0.9392996780340163\n",
            "\n",
            "Test precision_micro: 0.9392996780340163\n",
            "\n",
            "Test precision_macro: 0.824835962947899\n",
            "\n",
            "Test precision_weighted: 0.9352147450808641\n",
            "\n",
            "Test recall_micro: 0.9392996780340163\n",
            "\n",
            "Test recall_macro: 0.8228186234964372\n",
            "\n",
            "Test recall_weighted: 0.9392996780340163\n",
            "\n",
            "Test f1_micro: 0.9392996780340163\n",
            "\n",
            "Test f1_macro: 0.8185204903829856\n",
            "\n",
            "Test f1_weighted: 0.9334552542566866\n",
            "\n",
            "Epoch [9 / 10]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over train batches: 100%|██████████| 7493/7493 [00:50<00:00, 149.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.031426251218662216\n",
            "\n",
            "Train accuracy: 0.9945965556963423\n",
            "\n",
            "Train precision_micro: 0.9945965556963423\n",
            "\n",
            "Train precision_macro: 0.9759013382249141\n",
            "\n",
            "Train precision_weighted: 0.9946866802647841\n",
            "\n",
            "Train recall_micro: 0.9945965556963423\n",
            "\n",
            "Train recall_macro: 0.9747648968134096\n",
            "\n",
            "Train recall_weighted: 0.9945965556963423\n",
            "\n",
            "Train f1_micro: 0.9945965556963423\n",
            "\n",
            "Train f1_macro: 0.9738737810650661\n",
            "\n",
            "Train f1_weighted: 0.9941860103572447\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over test batches: 100%|██████████| 3465/3465 [00:13<00:00, 262.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss:  0.33746273782911235\n",
            "\n",
            "Test accuracy: 0.9399046847944048\n",
            "\n",
            "Test precision_micro: 0.9399046847944048\n",
            "\n",
            "Test precision_macro: 0.8256264001449959\n",
            "\n",
            "Test precision_weighted: 0.9291468802262332\n",
            "\n",
            "Test recall_micro: 0.9399046847944048\n",
            "\n",
            "Test recall_macro: 0.8243693740265933\n",
            "\n",
            "Test recall_weighted: 0.9399046847944048\n",
            "\n",
            "Test f1_micro: 0.9399046847944048\n",
            "\n",
            "Test f1_macro: 0.8198679750144439\n",
            "\n",
            "Test f1_weighted: 0.9308546889713838\n",
            "\n",
            "Epoch [10 / 10]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over train batches: 100%|██████████| 7493/7493 [00:50<00:00, 149.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.02720026347420171\n",
            "\n",
            "Train accuracy: 0.9957068354507308\n",
            "\n",
            "Train precision_micro: 0.9957068354507308\n",
            "\n",
            "Train precision_macro: 0.9808907076253579\n",
            "\n",
            "Train precision_weighted: 0.9956888672824492\n",
            "\n",
            "Train recall_micro: 0.9957068354507308\n",
            "\n",
            "Train recall_macro: 0.9800616303251853\n",
            "\n",
            "Train recall_weighted: 0.9957068354507308\n",
            "\n",
            "Train f1_micro: 0.9957068354507308\n",
            "\n",
            "Train f1_macro: 0.9793454249903208\n",
            "\n",
            "Train f1_weighted: 0.995330686137321\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over test batches: 100%|██████████| 3465/3465 [00:13<00:00, 263.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss:  0.3293513215063583\n",
            "\n",
            "Test accuracy: 0.9398166063099648\n",
            "\n",
            "Test precision_micro: 0.9398166063099648\n",
            "\n",
            "Test precision_macro: 0.8308692188856077\n",
            "\n",
            "Test precision_weighted: 0.9333200482802102\n",
            "\n",
            "Test recall_micro: 0.9398166063099648\n",
            "\n",
            "Test recall_macro: 0.8295870095602489\n",
            "\n",
            "Test recall_weighted: 0.9398166063099648\n",
            "\n",
            "Test f1_micro: 0.9398166063099648\n",
            "\n",
            "Test f1_macro: 0.8251690225475162\n",
            "\n",
            "Test f1_weighted: 0.9330719974342092\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "train_dataset = NERDataset(\n",
        "    token_seq=train_token_seq,\n",
        "    label_seq=train_label_seq,\n",
        "    token2idx=token2idx,\n",
        "    label2idx=label2idx,\n",
        ")\n",
        "valid_dataset = NERDataset(\n",
        "    token_seq=valid_token_seq,\n",
        "    label_seq=valid_label_seq,\n",
        "    token2idx=token2idx,\n",
        "    label2idx=label2idx,\n",
        ")\n",
        "test_dataset = NERDataset(\n",
        "    token_seq=test_token_seq,\n",
        "    label_seq=test_label_seq,\n",
        "    token2idx=token2idx,\n",
        "    label2idx=label2idx,\n",
        ")\n",
        "\n",
        "collator = NERCollator(\n",
        "    token_padding_value=token2idx[\"<PAD>\"],\n",
        "    label_padding_value=-1,\n",
        ")\n",
        "\n",
        "valid_dataloader = torch.utils.data.DataLoader(\n",
        "    valid_dataset,\n",
        "    batch_size=1,  # для корректных замеров метрик оставить batch_size=1\n",
        "    shuffle=False, # для корректных замеров метрик оставить shuffle=False\n",
        "    collate_fn=collator,\n",
        ")\n",
        "test_dataloader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=1,  # для корректных замеров метрик оставить batch_size=1\n",
        "    shuffle=False, # для корректных замеров метрик оставить shuffle=False\n",
        "    collate_fn=collator,\n",
        ")\n",
        "\n",
        "# ### START CODE HERE ###\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=False,\n",
        "    collate_fn=collator\n",
        ") \n",
        "\n",
        "model = BiLSTMAttn(\n",
        "    num_embeddings=len(token2idx),\n",
        "    embedding_dim=300,\n",
        "    hidden_size=256,\n",
        "    num_layers=1,\n",
        "    dropout=0,\n",
        "    bidirectional=False,\n",
        "    n_classes=len(label2idx),\n",
        "    token_padding_value=token2idx[\"<PAD>\"],\n",
        "    max_norm=None,\n",
        "    num_heads=8\n",
        ").to(device)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
        "writer = SummaryWriter(log_dir=f\"logs/BiLSTMAttn\")\n",
        "train(n_epochs=10,\n",
        "      model=model,\n",
        "      train_dataloader=train_dataloader,\n",
        "      valid_dataloader=valid_dataloader,\n",
        "      optimizer=optimizer,\n",
        "      criterion=criterion,\n",
        "      writer=writer,\n",
        "      device=device,\n",
        "      model_type = 'BiLSTM')\n",
        "# ### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 19659), started 0:00:14 ago. (Use '!kill 19659' to kill it.)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-fb12a5d898536a54\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-fb12a5d898536a54\");\n",
              "          const url = new URL(\"http://localhost\");\n",
              "          const port = 6006;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs_tosend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 401,
      "metadata": {
        "id": "MvESUF7FVAjD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over test batches: 100%|██████████| 3683/3683 [00:14<00:00, 256.30it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss:  0.5563465393900795\n",
            "\n",
            "Test accuracy: 0.9030084513790352\n",
            "\n",
            "Test precision_micro: 0.9030084513790352\n",
            "\n",
            "Test precision_macro: 0.7854298149715907\n",
            "\n",
            "Test precision_weighted: 0.8953598945320568\n",
            "\n",
            "Test recall_micro: 0.9030084513790352\n",
            "\n",
            "Test recall_macro: 0.7884000380091156\n",
            "\n",
            "Test recall_weighted: 0.9030084513790352\n",
            "\n",
            "Test f1_micro: 0.9030084513790352\n",
            "\n",
            "Test f1_macro: 0.7808389399201645\n",
            "\n",
            "Test f1_weighted: 0.8939088439546743\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "evaluate_epoch(\n",
        "    model=model,\n",
        "    dataloader=test_dataloader,\n",
        "    criterion=criterion,\n",
        "    writer=writer,\n",
        "    device=device,\n",
        "    epoch=1,\n",
        "    model_type='BiLSTM'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Kачество не получилось побить даже в случае с BiLSTM, поскольку, видимо, BiLSTM и Attention получают контексты, но разные и их \n",
        "смешивание (после BiLSTM -> Attention) не дает улучшения."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpMp9p8xcoeG"
      },
      "source": [
        "## Бонус 2: ChatGPT-теггер (2 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffRpFB-ZbRsr"
      },
      "source": [
        "Творческое задание, в котором Вам требуется использовать ChatGPT от OpenAI для разметки именованных сущностей. В этой части Вы вольны использовать любые приемы и ухищрения, чтобы заставить модель классифицировать токены.\n",
        "\n",
        "Ваше задание заключается в следующем:\n",
        "- с помощью ChatGPT разметить первые 30 объектов из test_token_seq;\n",
        "- на размеченных объектах посчитать качество с помощью ранее описанной функции `compute_metrics`;\n",
        "- написать выводы.\n",
        "\n",
        "Один из возможных вариантов, но не единственный  -- использование техники Few-Shot Learning. Суть заключается в том, что модели нужно скормить на вход \"правила игры\", то есть то, что мы будем подавать на вход и что мы ожидаем на выходе. Например:\n",
        "\n",
        "\"eu rejects german call to boycott british lamb . -> B-ORG O B-MISC O O O B-MISC O O\"\n",
        "\n",
        "\"peter blackburn -> B-PER I-PER\"\n",
        "\n",
        "\"the european commission said on thursday it disagreed with german advice to consumers to shun british lamb until scientists determine whether mad cow disease can be transmitted to sheep . -> O B-ORG I-ORG O O O O O O B-MISC O O O O O B-MISC O O O O O O O O O O O O O O\"\n",
        "\n",
        "\"my name is lomonosov ->\" (и тут просим модель выдать ответ).\n",
        "\n",
        "Так делаем для первых 30 последовательностей из тестовой части и считаем метрики качества (размера батча при этом также равен одному).\n",
        "\n",
        "Здесь есть несколько нюансов, которые мы раскрывать не будем. Вам предстоит столкнуться с ними в процессе. Также мы намеренно не предоставляем дополнительных подсказок, предлагая полную свободу действий. Любые нестандартные техники и идеи приветствуются и будут поощераться баллами.\n",
        "\n",
        "**Важные детали**:\n",
        "- Вам нужно зарегистрировать аккаунт в системе OpenAI, лучше всего делать это через Gmail (домен @mail.ru, например, банится и не регистрируется).\n",
        "- Также Вам понадобиться VPN, без него по некоторым причинам не получится зайти на сайт, зарегистрироваться и воспользоваться моделью.\n",
        "- У Вас есть лимит на количество токенов, которые Вы можете обработать, поэтому расходуйте ресурс разумно. Но Вы можете регистрировать несколько аккаутов, пополнять баланс, использовать более \"дорогие\" модели -- здесь на Ваш выбор.\n",
        "- Основной целью этой части задания является показать, что LLM также можно использовать разметке именнованных сущностей. Так как техник очень много, мы предлагаем ориентироваться на порог качества **0.70** и выше по `f1-macro`. Этот порог можно достичь на стандартной версии `gpt-3.5-turbo`, без дополнительных денежных трат, ограничиваясь бесплатным лимитом.\n",
        "- Напишите содержательный вывод и Ваше мнение о целесобразности такого подхода, в чем его преимущества и недостатки, в каких ситуациях он имеет место быть, а где лучше использовать стандартные LSTM/Transformer-модели."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqcQWCuz2ZnH"
      },
      "outputs": [],
      "source": [
        "!pip install openai==0.28.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opn_4LeNTryx"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "openai.api_key = \"YOUR_TOKEN\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xuGUfPtPpKs"
      },
      "outputs": [],
      "source": [
        "# ### START CODE HERE ###\n",
        "...\n",
        "# ### END CODE HERE ###"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "4MIrbmNoQEMA"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "8d4ce941904148077feb793883e611d25d231ca995d9164b22ee99fd0facd8d1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
